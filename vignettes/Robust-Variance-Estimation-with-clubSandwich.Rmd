---
title: "Cluster-robust variance estimation with clubSandwich"
author: "James E. Pustejovsky and Hanna Kim"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    number_sections: true
bibliography: bibliography.bib
vignette: >
  %\VignetteIndexEntry{Cluster-robust variance estimation with clubSandwich}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

$$
\newcommand{\Prob}{\text{Pr}}
\newcommand{\E}{\text{E}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\corr}{\text{corr}}
\newcommand{\Var}{\text{Var}}
\newcommand{\tr}{\text{tr}}
\newcommand{\bmat}[1]{\mathbf{#1}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\half}[1]{#1^{\frac{1}{2}}}
\newcommand{\halfinv}[1]{#1^{-\frac{1}{2}}}
$$

Linear regression models, estimated by ordinary least squares weighted least squares, are a ubiquitous tool across a diverse range of fields. 
Classically, inference in such models is based on the assumption that the model's errors are independent and homoskedastic (or more generally, that the errors follow a known structure of a low-dimensional parameter). 
However, such assumptions will be unreasonable in many applications. 
For instance, using the classical assumptions can lead to underestimated standard errors when observations are drawn from groups or clusters that are subject to shared sources of error.
Cluster-robust variance estimation methods provide a basis for inference under the weaker assumption that the errors of observations from different clusters are independent but errors within a given cluster may be correlated. 
These methods can be applied across a broad scope of models, including not only ordinary linear regression, but also hierarchical (random effects) linear models, meta-analytic models, generalized linear models, generalized estimating equations, and instrumental variables models. 

Let us consider the linear regression model
$$
\begin{aligned}
y_{ij} &= \beta_0 + \beta_1 x_{1ij} + \beta_2 x_{2ij} + \cdots + \beta_{p-1} x_{p-1,ij} + e_{ij}, \\
 &= \mathbf{x}_{ij} \boldsymbol\beta + e_{ij}
\end{aligned}
$$

where $\mathbf{x}_{ij}$ is a $1 \times p$ row vector of predictors (possibly including an intercept term) and $e_{ij}$ is an error term, both for observation $i = 1,...,n_j$ in cluster $j = 1,...,J$. It will be convenient to write this model using vectors and matrices for each cluster, as
$$
\mathbf{y}_j = \mathbf{X}_j \boldsymbol\beta + \mathbf{e}_j,
$$
where $\mathbf{y}_j = (y_{1j} \cdots y_{n_j j})'$ is the $n_j \times 1$ vector of outcomes for cluster $j$, $\mathbf{X}_j = (\mathbf{x}_{1j}' \cdots \mathbf{x}_{n_j j}')'$ is the $n_j \times p$ matrix of predictors for cluster $j$, and $\mathbf{e}_j$ is the $n_j \times 1$ vector of errors for cluster $j$, all for $j = 1,...,J$. 

We will maintain the following assumptions regarding the errors in the regression:
$$
\begin{aligned}
A1:& \qquad \E(\bmat{e}_j | \bmat{X}_j) = \bmat{0} \\ 
A2:& \qquad \E(\bmat{e}_j \bmat{e}_{k}' | \bmat{X}_j, \bmat{X}_{k}) = \bmat{0}, \qquad \text{when} \qquad j \neq {k} \\
A3:& \qquad \E(\bmat{e}_j \bmat{e}_j' | \bmat{X}_j) = \Var(\bmat{e}_j) = \bs\Omega_j.
\end{aligned}
$$
The $n_j \times n_j$ matrices $\bs\Omega_j$ capture the cluster-specific variance-covariance matrix of the errors.
A common strategy for estimation and inference about the regression coefficients is to make further assumptions regarding the form of these matrices, such as by assuming that all errors are independent and homoskedastic (i.e., $\bs\Omega_j = \sigma^2 \bmat{I}_j$) or that they have a structure characterized by a low-dimensional parameter (as in a hierarchical linear model or random effects model). 
Robust variance estimation methods provide a method for characterizing uncertainty and conducting inference without relying on such additional assumptions. 
Rather, we will treat the variance-covariance matrices as following an unknown form. 
Thus, the errors within a cluster need not be homoskedastic or independent, but can instead be arbitrarily correlated within the same cluster.

Researchers will often estimate the regression coefficient vector $\bs\beta$ using ordinary least squares (OLS) or, more generally, some form of weighted least squares (WLS). The weighted least squares estimator might involve non-unit weights for each observation or some form of weight matrices for each cluster, potentially including off-diagonal terms (as in a hierarchical linear model). 
We will consider a general set of symmetric weight matrices $\bmat{W}_1,...,\bmat{W}_J$.
The weighted least squares estimator for $\bs\beta$ is then defined as
$$
\bs{\hat\beta} = \bmat{M} \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bmat{y}_j\right), \quad \text{where} \quad \bmat{M} = \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bmat{X}_j\right)^{-1}.
$$

In some applications, the weight matrices used for WLS estimation are motivated by a working model for the structure of the error variance-covariances $\bs\Omega_j$. 
In this approach, the analyst posits a structure $\bs\Omega_j = \bs\Psi_j(\bs\theta)$ for some low-dimensional, estimable parameter $\bs\theta$. 
If the working model captures the true form of the error variances, then the weights that make the sampling variance of $\bs{\hat\beta}$ as small as possible (the most precise) are the inverse variance weights $\bs\Psi_j^{-1}$, which depend on the parameter $\bs\theta$. 
In practice, the variance parameter $\bs\theta$ is unknown and so the analyst will use the inverse of the _estimated_ working model as the weights, with $\bmat{W}_j = \bs{\hat\Psi}_j^{-1}$. When the weight matrices are so derived, we will refer to them as "inverse-variance" weights.

# Robust Variance Estimation

Treating the weight matrices as fixed, the sampling variance of $\bs{\hat\beta}$ is 
$$
\Var\left(\bs{\hat\beta}\right) = \bmat{M} \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bs\Omega_j \bmat{W}_j \bmat{X}_j \right) \bmat{M},
$$
which depends on the unknown variance-covariances $\bs\Omega_1,...,\bs\Omega_J$. One strategy for estimating $\Var\left(\bs{\hat\beta}\right)$ is to substitute an estimated working model $\bs{\hat\Psi}_j$ for the unknown variance-covariance matrices $\bs\Omega_j$, giving a model-based variance estimator:
$$
\bmat{V}^{model} = \bmat{M} \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bs{\hat\Psi}_j \bmat{W}_j \bmat{X}_j \right) \bmat{M}.
$$
If the working model is correctly specified and precisely estimated, then the model-based variance estimator will closely approximate the true sampling variance. If the analyst uses inverse-variance weights based on the same working model, then the model-based variance estimator has the simpler form $\bmat{V}^{model} = \bmat{M}$. However, if the working model is inaccurate or incorrectly specified in some way, then there is no guarantee that the model-based variance estimator will be valid. 

### CR0

<!-- 11/3: Cite White (1985), Liang & Zeger (1986) as source for CR0 --> 

The original form of the robust variance estimator uses the residuals from each cluster to derive rough estimates of the unknown error variance matrices. This estimator substitutes the cross product of the residuals $\bmat{\hat{u}}_j\bmat{\hat{u}}_j'$ in place of $\bs\Omega$ (or $\bs{\hat\Psi}_j$), leading to the variance estimator
$$
\bmat{V}^{CR0} = \bmat{M} \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bmat{\hat{u}}_j \bmat{\hat{u}}_j' \bmat{W}_j\bmat{X}_j\right) \bmat{M}.
$$
Each 

<!-- H: Should we number the equations to refer to them? -->

<!-- 9/21: We might add some brief sentences about the limitations of CR0 referring to Cameron & Miller (2015) or others. -->
<!-- H: Briefly explain what problems could arise by using CR0 for cases with a small number of clusters. -->


## Small-sample corrections

A number of variations on the basic cluster-robust variance estimator have been proposed, all of which aim to provide more accurate approximations when the number of clusters is limited. 
The variations implemented in the `clubSandwich` package all have the form
$$
\bmat{V}^{CR} = f \times \bmat{M} \left(\sum_{j=1}^J \bmat{E}_j' \bmat{\hat{u}}_j \bmat{\hat{u}}_j' \bmat{E}_j \right) \bmat{M}
$$
for some constant $f$ and some estimating matrices $\bmat{E}_1,...,\bmat{E}_J$. With this representation, the CRO estimator corresponds to $f_0 = 1$ and $\bmat{E}^{CR0}_j = \bmat{W}_j \bmat{X}_j$.


### CR1

The CR1, CR1S, and CR1p estimators are variants that use the following constants with $\bmat{E}^{CR1}_j = \bmat{W}_j \bmat{X}_j$.
$$
\begin{aligned}
\text{CR1}: f_1 &= \frac{J}{J - 1}, \\
\text{CR1S}: f_{1S} &= \frac{J (N - 1)}{(J - 1)(N - p)} , \\
\text{CR1p}: f_{1p} &= \frac{J}{J - p}, 
\end{aligned}
$$

These corrections are all motivated by the observation that the sandwich estimator corresponds to a sample variance-covariance matrix across terms for each cluster. Thus, inflating it by an amount depending on the number of clusters should tend to reduce the small-sample bias of CR0. However, the exact bias of CR0 depends on features of the data beyond simply the number of clusters and predictors, including the distribution of cluster sizes and the distribution of the predictors within and across clusters [@mccaffrey2001generalizations;@tipton2015small;@pustejovsky2018small]. Thus, the simple multiplicative corrections involved in the CR1 and related estimators are only very rough approximations.


### CR2

@bell2002bias and @mccaffrey2001generalizations introduced a class of more refined robust variance estimators, termed "bias-reduced linearization" estimators, that account for variation in cluster sizes and the distribution of predictors. These estimators require positing a working model for the error structure, which might or might not also be used to derive the weight matrices. Based on this working model, the estimators incorporate adjustments to the residuals used in the middle of the sandwich estimator, such that the whole estimator is exactly unbiased when the working model is correctly specified.

The CR2 estimator uses $f_2 = 1$ and
$$
\bmat{E}^{CR2}_j = \bmat{A}_j' \bmat{W}_j \bmat{X}_j,
$$
for $j=1,...,J$, where $\bmat{A}_1,...,\bm{A}_J$ are adjustment matrices that satisfy the criteria
$$
\E\left(\bmat{A}_j\bmat{\hat{u}}_j \bmat{\hat{u}}_j' \bmat{A}_j'\right) = \bs\Psi_j.
$$
where $\bs\Psi_j$ denotes the working model for cluster $j$ and the expectation is taken assuming that the working model is correct. Thus, if the working model is correctly specified,
$$
\E\left({\bmat{E}^{CR2}_j}'\bmat{\hat{u}}_j \bmat{\hat{u}}_j' \bmat{E}_j^{CR2}\right) = \bmat{X}_j'\bmat{W}_j \bs\Psi_j \bmat{W}_j \bmat{X}_j,
$$
and so $\E\left(\bmat{V}^{CR2}\right) = \Var\left(\bs{\hat\beta}\right)$.

To derive expressions for $\bmat{A}_j$, observe that the residuals from cluster $j$ may be written as $\bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bmat{Y}$, where $\bmat{H} = \bmat{X}\bmat{M}\bmat{X}'\bmat{W}$ and $\bmat{C}_j$ is an $n_j \times N$ matrix that selects the rows of the full data that correspond to cluster $j$ (i.e., $\bmat{X}_j = \bmat{C}_j \bmat{X}$). Because the residuals have mean zero, the expectation of the cross-product of residuals from cluster $j$ is
$$
\E\left(\bmat{\hat{u}}_j \bmat{\hat{u}}_j'\right) = \bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j.
$$
The adjustment matrices therefore satisfy the criteria
$$
\bmat{A}_j \bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j \bmat{A}_j' = \bs\Psi_j
$$
for $j = 1,...,J$. There are multiple possible solutions to the above. @mccaffrey2001generalizations considered several possible solutions to the above. A symmetric solution is given by 
$$
\bmat{A}_j = \bmat{D}_j' \halfinv{\bmat{B}}_j \bmat{D}_j,
$$
where $\bmat{D}_j$ is the upper-right triangular Cholesky factorization of $\bs\Psi_j$, so that $\bmat{D}_j' \bmat{D}_j = \bs\Psi_j$,  
$$
\bmat{B}_j = \bmat{D}_j \bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j \bmat{D}_j',
$$
and $\halfinv{\bmat{B}}_j$ is the inverse of the symmetric square root of $\bmat{B}_j$. 
If the weight matrices are inverse-variance, then the matrix $\bmat{B}_j$ has the simpler form
$$
\bmat{B}_j = \bmat{D}_j \left(\bs\Psi_j - \bmat{X}\bmat{M}\bmat{X}'\right)\bmat{D}_j'.
$$
For the special case of ordinary least squares, the adjustment matrices simplify even further to
$$
\bmat{A}_j = \halfinv{\left(\bmat{I}_j - \bmat{X}_j \bmat{M}\bmat{X}_j'\right)}
$$
if the working model is based on the assumption of independent, homoskedastic errors. 

@mccaffrey2001generalizations also considered an asymmetric solution to the criteria. However, they recommended the symmetric one because it minimizes the squared error between the OLS residuals and the the true errors when the errors are i.i.d. Even when the errors were not independent, @mccaffrey2001generalizations found that the symmetric adjustment matrices yielded smaller bias than asymmetric ones.

@niccodemi2020refining described an efficient method of calculating the CR2 for the special case of ordinary least squares with an independence working model...

Unfortunately, their computational approach does not generalize to the CR2 adjustment matrices for weighted least squares or working models other than $\Psi_j = \sigma^2 \bmat{I}_j$. 


### CR2f

@mccaffrey2001generalizations also considered another approach to correcting the bias of the residuals, using a different set of adjustment matrices that we term CR2f. The CR2f estimator uses $f_2 = 1$ and
$$
\bmat{E}^{CR2f}_j = \half{\bmat{W}}_j \bmat{A}_j \half{\bmat{W}}_j \bmat{X}_j,
$$
where $\bmat{A}_1,...,\bmat{A}_j$ are adjustment matrices that satisfy the criteria
$$
\E\left(\bmat{A}_j\half{\bmat{W}}_j\bmat{\hat{u}}_j \bmat{\hat{u}}_j'\half{\bmat{W}}_j \bmat{A}_j'\right) = \bmat{A}_j \half{\bmat{W}}_j \bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j \half{\bmat{W}}_j \bmat{A}_j' = \half{\bmat{W}}_j \bs\Psi_j\half{\bmat{W}}_j.
$$
Following the same reasoning as with the CR2 estimator, adjustment matrices that satisfy these criteria lead to a variance estimator that is exactly unbiased if the working model is correctly specified. A symmetric solution is given by 
$$
\bmat{A}_j = \bmat{D}_j' \halfinv{\bmat{B}}_j \bmat{D}_j,
$$
where $\bmat{D}_j$ is the upper-right triangular Cholesky factorization of $\bs\Psi_j$ and   
$$
\bmat{B}_j = \bmat{D}_j \half{\bmat{W}}_j\bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j \half{\bmat{W}}_j \bmat{D}_j'.
$$
If the weight matrices are inverse-variance, then the CR2f adjustment matrices have the simpler form
$$
\bmat{A}_j = \halfinv{\left(\bmat{I}_j - \half{\bmat{W}}_j\bmat{X}\bmat{M}\bmat{X}'\half{\bmat{W}}_j\right)}.
$$
For ordinary (unweighted) least squares, the CR2 and CR2f adjustment matrices are equivalent.

### CR3 

- CR3 uses $f_3 = 1$ and 
$$
\bmat{E}_j = \bmat{A}_j \bmat{W}_j \bmat{X}_j,
$$
where $\bmat{A}_j$ are adjustment matrices...

<!-- Jackknife - See Tipton (2015) for some arguments -->

### CR4 

- CR4 uses $f_4 = 1$ and
$$
\bmat{E}_j = \bmat{W}_j \bmat{X}_j \bmat{D}_j,
$$
where $\bmat{D}_j$ are adjustment matrices...


# Model-specific considerations

## `lm()`

## `mlm()`

## `nlme::gls()`, `nlme::lme()`, and `lme4::lmer()`

## `plm::plm()`

## `glm()` and `geepack::geeglm()`

## `robumeta::robu()`

## `metafor::rma.uni()` and `metafor::rma.mv()`

# Single-coefficient hypothesis tests and confidence intervals

<!-- (To use the Satterthwaite approximation (instead of the Wald-test assuming large samples), we need the mean and variance of $V$. These are generally unknown (because they depend on $\bs\Omega$), but we can approximate them by assuming that the working model is correct. - H: I think this is the key idea/motivation behind cluster-robust variance estimation. -->

## Degrees of freedom corrections

- z
- naive-t, naive-tp
- Satterthwaite correction

## Saddlepoint correction

## `coef_test()`

## `conf_int()`

## `linear_contrast()`


# Multiple-constraint hypothesis tests

- `Wald_test()`

## Convenience functions for constraint matrices

- `constrain_equal()`
- `constrain_zero()`
- `constrain_pairwise()`

# Computational considerations

# References {-}
