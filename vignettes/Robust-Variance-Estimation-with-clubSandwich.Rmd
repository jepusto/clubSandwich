---
title: "Cluster-robust variance estimation with clubSandwich"
author: "James E. Pustejovsky and Hanna Kim"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    number_sections: true
bibliography: bibliography.bib
vignette: >
  %\VignetteIndexEntry{Cluster-robust variance estimation with clubSandwich}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

$$
\newcommand{\Prob}{\text{Pr}}
\newcommand{\E}{\text{E}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\corr}{\text{corr}}
\newcommand{\Var}{\text{Var}}
\newcommand{\tr}{\text{tr}}
\newcommand{\bmat}[1]{\mathbf{#1}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\half}[1]{#1^{\frac{1}{2}}}
\newcommand{\halfinv}[1]{#1^{-\frac{1}{2}}}
\newcommand{\sb}[1]{{\{#1\}}}
$$

Linear regression models, estimated by ordinary least squares weighted least squares, are a ubiquitous tool across a diverse range of fields. 
Classically, inference in such models is based on the assumption that the model's errors are independent and homoskedastic (or more generally, that the errors follow a known structure of a low-dimensional parameter). 
However, such assumptions will be unreasonable in many applications. 
For instance, using the classical assumptions can lead to underestimated standard errors when observations are drawn from groups or clusters that are subject to shared sources of error.
Cluster-robust variance estimation methods provide a basis for inference under the weaker assumption that the errors of observations from different clusters are independent but errors within a given cluster may be correlated. 
These methods can be applied across a broad scope of models, including not only ordinary linear regression, but also hierarchical (random effects) linear models, meta-analytic models, generalized linear models, generalized estimating equations, and instrumental variables models. 

Let us consider the linear regression model
$$
\begin{aligned}
y_{ij} &= \beta_0 + \beta_1 x_{1ij} + \beta_2 x_{2ij} + \cdots + \beta_{p-1} x_{p-1,ij} + u_{ij}, \\
 &= \mathbf{x}_{ij} \boldsymbol\beta + u_{ij}
\end{aligned}
(\#eq:reg-model)
$$
where $\mathbf{x}_{ij}$ is a $1 \times p$ row vector of predictors (possibly including an intercept term) and $u_{ij}$ is an error term, both for observation $i = 1,...,n_j$ in cluster $j = 1,...,J$. It will be convenient to write \@ref(eq:reg-model) using vectors and matrices for each cluster, as
$$
\mathbf{y}_j = \mathbf{X}_j \boldsymbol\beta + \bmat{u}_j,
(\#eq:reg-matrix)
$$
where $\mathbf{y}_j = (y_{1j} \cdots y_{n_j j})'$ is the $n_j \times 1$ vector of outcomes for cluster $j$, $\mathbf{X}_j = (\mathbf{x}_{1j}' \cdots \mathbf{x}_{n_j j}')'$ is the $n_j \times p$ matrix of predictors for cluster $j$, and $\bmat{u}_j$ is the $n_j \times 1$ vector of errors for cluster $j$, all for $j = 1,...,J$. 

We will maintain the following assumptions regarding the errors in the regression:
$$
\begin{aligned}
A1:& \qquad \E(\bmat{u}_j | \bmat{X}_j) = \bmat{0} \\ 
A2:& \qquad \E(\bmat{u}_j \bmat{u}_{k}' | \bmat{X}_j, \bmat{X}_{k}) = \bmat{0}, \qquad \text{when} \qquad j \neq {k} \\
A3:& \qquad \E(\bmat{u}_j \bmat{u}_j' | \bmat{X}_j) = \Var(\bmat{u}_j) = \bs\Omega_j.
\end{aligned}
(\#eq:reg-assumptions)
$$
The $n_j \times n_j$ matrices $\bs\Omega_j$ are the cluster-specific variance-covariance matrices of the errors. 
A common strategy for estimation and inference about the regression coefficients is to make further assumptions regarding the form of these matrices, such as by assuming that all errors are independent and homoskedastic (i.e., $\bs\Omega_j = \sigma^2 \bmat{I}_j$) or that they have a structure characterized by a low-dimensional parameter (as in a hierarchical linear model or random effects model). 
Robust variance estimation methods provide a method for assessing uncertainty and conducting inference without relying on such additional assumptions. 
Rather, we will treat the variance-covariance matrices as following an unknown form. 
Thus, the errors within a cluster need not be homoskedastic or independent, but can instead be arbitrarily correlated within the same cluster. 

Researchers will often estimate the regression coefficient vector $\bs\beta$ using ordinary least squares (OLS) or, more generally, some form of weighted least squares (WLS). 
The weighted least squares estimator might involve non-unit weights for each observation or some form of weight matrices for each cluster, potentially including off-diagonal terms (as in a hierarchical linear model). 
We will consider a general set of symmetric weight matrices $\bmat{W}_1,...,\bmat{W}_J$. 
The weighted least squares estimator for $\bs\beta$ is then defined as
$$
\bs{\hat\beta} = \bmat{M} \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bmat{y}_j\right), \quad \text{where} \quad \bmat{M} = \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bmat{X}_j\right)^{-1}.
(\#eq:WLS-M)
$$
In some applications, the weight matrices used for WLS estimation are motivated by a working model for the structure of the error variance-covariances $\bs\Omega_j$. 
In this approach, the analyst posits a structure $\bs\Omega_j = \bs\Psi_j(\bs\theta)$ for some low-dimensional, estimable parameter $\bs\theta$. 
If the working model captures the true form of the error variances, then the weights that make the sampling variance of $\bs{\hat\beta}$ as small as possible (the most precise) are the inverse of the variance-covariance matrix of the errors, which depend on the parameter $\bs\theta$. 
In practice, the variance parameter $\bs\theta$ is unknown and so the analyst will use the inverse of the _estimated_ working model as the weights, with $\bmat{W}_j = \bs{\hat\Psi}_j^{-1} = \bs\Psi_j^{-1}(\bs{\hat\theta})$. 
When the weight matrices are derived following this strategy, we will refer to them as "inverse-variance" weights. 


# Robust Variance Estimation

Treating the weight matrices as fixed, the sampling variance of $\bs{\hat\beta}$ is 
$$
\Var\left(\bs{\hat\beta}\right) = \bmat{M} \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bs\Omega_j \bmat{W}_j \bmat{X}_j \right) \bmat{M},
(\#eq:var-beta-hat)
$$
which depends on the unknown variance-covariances $\bs\Omega_1,...,\bs\Omega_J$. 
One strategy for estimating \@ref(eq:var-beta-hat) is to substitute an _estimated_ working model $\bs{\hat\Psi}_j$ for the unknown variance-covariance matrices $\bs\Omega_j$, giving a model-based variance estimator:
$$
\bmat{V}^M = \bmat{M} \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bs{\hat\Psi}_j \bmat{W}_j \bmat{X}_j \right) \bmat{M}.
(\#eq:V-model)
$$
If the working model is correctly specified and precisely estimated, then the model-based variance estimator will closely approximate the true sampling variance. 
If the analyst uses inverse-variance weights based on the same working model, then the model-based variance estimator has the simpler form $\bmat{V}^{M} = \bmat{M}$. 
However, if the working model is inaccurate or incorrectly specified in some way, then there is no guarantee that the model-based variance estimator will be valid. 

The original form of the robust variance estimator uses the residuals 
$$
\bmat{\hat{u}}_j = \mathbf{y}_j - \mathbf{X}_j \bs{\hat\beta}
(\#eq:reg-residuals)
$$
from each cluster to derive rough estimates of the unknown error variance matrices. 
Specifically, the CR0 estimator substitutes the cross product of the residuals $\bmat{\hat{u}}_j\bmat{\hat{u}}_j'$ in place of $\bs\Omega_j$ in \@ref(eq:var-beta-hat), leading to 
$$
\bmat{V}^{CR0} = \bmat{M} \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bmat{\hat{u}}_j \bmat{\hat{u}}_j' \bmat{W}_j\bmat{X}_j\right) \bmat{M}.
(\#eq:V-CR0)
$$
Even though the cross-product of the residuals from a given cluster is a very crude estimator for the error variance matrix from that cluster, it serves the purpose because we do not need to estimate each cluster-specific variance matrix accurately. 
Rather, we only need to approximate the _sum_ of the terms involving the error variance matrix, and even the crude CR0 estimator works for this purpose if the number of independent clusters is sufficiently large. 

However, it may happen that the number of clusters is not large enough to trust the asymptotic approximation, such that \@ref(eq:V-CR0) underestimates the sampling variance of $\bs{\hat\beta}$ and leads to larger than nominal Type I error rates [@cameron2015practitioner]. 
The over-rejection problem will be discussed further in detail in section \@ref(sec:t-test). 
It should be noted that the number of clusters deemed as "few" is not clear-cut and depends on specific study conditions, requiring more clusters when the clusters are unbalanced [@cameron2015practitioner]. 

## Small-sample corrections

A number of variations on the basic cluster-robust variance estimator have been proposed, all of which aim to provide more accurate approximations when the number of clusters is limited. 
The variations implemented in the `clubSandwich` package all have the form
$$
\bmat{V}^{CR} = f \times \bmat{M} \left(\sum_{j=1}^J \bmat{E}_j' \bmat{\hat{u}}_j \bmat{\hat{u}}_j' \bmat{E}_j \right) \bmat{M}
(\#eq:V-CR)
$$
for some constant $f$ and some estimating matrices $\bmat{E}_1,...,\bmat{E}_J$. 
With this representation, the CRO estimator corresponds to $f_0 = 1$ and $\bmat{E}^\sb{0}_j = \bmat{W}_j \bmat{X}_j$. 

### CR1

The CR1, CR1S, and CR1p estimators are variants that use the following constants with $\bmat{E}^\sb{1}_j = \bmat{W}_j \bmat{X}_j$. 
$$
\begin{aligned}
\text{CR1}: f_1 &= \frac{J}{J - 1}, \\
\text{CR1S}: f_{1S} &= \frac{J (N - 1)}{(J - 1)(N - p)} , \\
\text{CR1p}: f_{1p} &= \frac{J}{J - p}, 
\end{aligned}
(\#eq:CR1)
$$
These corrections are all motivated by the observation that the sandwich estimator corresponds to a sample variance-covariance matrix across terms for each cluster. 
Thus, inflating it by an amount depending on the number of clusters should tend to reduce the small-sample bias of CR0. 
However, the exact bias of CR0 depends on features of the data beyond simply the number of clusters and predictors, including cluster sizes and the distribution of the predictors within and across clusters [@mccaffrey2001generalizations;@tipton2015small;@pustejovsky2018small]. 
Thus, the simple multiplicative corrections involved in the CR1 and related estimators are only very rough approximations, and do not provide adequate finite-cluster corrections except under very specific conditions [@cameron2015practitioner]. 

### CR2

@bell2002bias and @mccaffrey2001generalizations introduced a class of more refined robust variance estimators, termed "bias-reduced linearization" estimators, that account for variation in cluster sizes and the distribution of predictors. 
These estimators require positing a working model for the error structure, based on which the estimators incorporate adjustments to the residuals used in the middle of the sandwich estimator. 
The whole estimator is exactly unbiased when the working model is correctly specified. 
Although in principle a working model is required prior to the whole process, we rarely have a known working model for the error structure of regression coefficients. Therefore, in practice, we will use estimated working models, analogous to using frequentist model estimates as convenient priors for Bayesian estimation. 

The CR2 estimator uses $f_2 = 1$ and
$$
\bmat{E}^\sb{2}_j = {\bmat{A}^\sb{2}_j}' \bmat{W}_j \bmat{X}_j,
(\#eq:CR2-Ej)
$$
inside \@ref(eq:V-CR) for $j=1,...,J$, where $\bmat{A}^\sb{2}_1,...,\bmat{A}^\sb{2}_J$ are adjustment matrices that satisfy the criteria
$$
\E\left(\bmat{A}^\sb{2}_j\bmat{\hat{u}}_j \bmat{\hat{u}}_j' {\bmat{A}^\sb{2}_j}'\right) = \bs\Psi_j.
(\#eq:CR2-criterion)
$$
$\bs\Psi_j$ denotes the working model for cluster $j$ and the expectation is taken assuming that the working model is correct (i.e., correctly specified and ignoring any estimation error). 
Thus, if the working model is correctly specified, 
$$
\E\left({\bmat{E}^\sb{2}_j}'\bmat{\hat{u}}_j \bmat{\hat{u}}_j' \bmat{E}^\sb{2}_j\right) = \bmat{X}_j'\bmat{W}_j \bs\Psi_j \bmat{W}_j \bmat{X}_j,
(\#eq:CR2-expct)
$$
and so $\E\left(\bmat{V}^{CR2}\right) = \Var\left(\bs{\hat\beta}\right)$ by \@ref(eq:var-beta-hat) and \@ref(eq:V-CR). 

To derive expressions for $\bmat{A}^\sb{2}_j$, observe that the residuals from cluster $j$ may be written as $\bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bmat{Y}$, where $\bmat{H} = \bmat{X}\bmat{M}\bmat{X}'\bmat{W}$ and $\bmat{C}_j$ is an $n_j \times N$ matrix that selects the rows of the full data that correspond to cluster $j$ (i.e., $\bmat{X}_j = \bmat{C}_j \bmat{X}$). 
Because the residuals have mean zero, the expectation of the cross-product of residuals from cluster $j$ is
$$
\E\left(\bmat{\hat{u}}_j \bmat{\hat{u}}_j'\right) = \bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j'.
(\#eq:resid-expct)
$$
The adjustment matrices must therefore satisfy the criterion 
$$
\bmat{A}^\sb{2}_j \bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j' {\bmat{A}^\sb{2}_j}' = \bs\Psi_j
(\#eq:CR2-criterion2)
$$
for $j = 1,...,J$. 

There are multiple possible solutions to the above. @mccaffrey2001generalizations considered several possible solutions to the above, including a symmetric solution given by 
$$
\bmat{A}^\sb{2}_j = {\bs\Psi^C_j}' \halfinv{\left(\bmat{B}_j^\sb{2}\right)} \bs\Psi^C_j,
(\#eq:CR2-symm)
$$

where $\bs\Psi^C_j$ is the upper-right triangular Cholesky factorization of $\bs\Psi_j$, so that ${\bs\Psi^C_j}' \bs\Psi^C_j = \bs\Psi_j$, 
$$
\bmat{B}^\sb{2}_j = \bs\Psi^C_j \bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j' {\bs\Psi^C_j}',
(\#eq:CR2-symm-Bj)
$$
and $\halfinv{\left(\bmat{B}_j^\sb{2}\right)}$ is the inverse of the symmetric square root of $\bmat{B}^\sb{2}_j$. 

If the weight matrices are inverse-variance, then the matrix $\bmat{B}^\sb{2}_j$ has the simpler form
$$
\bmat{B}^\sb{2}_j = \bs\Psi^C_j \left(\bs\Psi_j - \bmat{X}_j\bmat{M}\bmat{X}_j'\right) {\bs\Psi^C_j}'.
(\#eq:CR2-inv-Bj)
$$
For the special case of ordinary least squares, the adjustment matrices simplify even further to 
$$
\bmat{A}^\sb{2}_j = \halfinv{\left(\bmat{I}_j - \bmat{X}_j \bmat{M}\bmat{X}_j'\right)},
(\#eq:CR2-OLS)
$$
if the working model is based on the assumption of independent, homoskedastic errors. 

@mccaffrey2001generalizations also considered an asymmetric solution to criterion \@ref(eq:CR2-criterion2). However, they recommended the symmetric one because it minimizes the squared error between the OLS residuals and the true errors when they are independent and homoskedastic. 
Even when the errors were not independent, they found that the symmetric adjustment matrices yielded smaller bias than asymmetric ones. 

@niccodemi2020refining described an efficient method of calculating the CR2 for the special case of ordinary least squares with an independence working model. 
Unfortunately, their computational approach does not generalize to the CR2 adjustment matrices for weighted least squares or working models other than $\Psi_j = \sigma^2 \bmat{I}_j$. 
However, it is applicable to the CR2f adjustment, as we describe subsequently. 

### CR2f

@mccaffrey2001generalizations also considered another approach to correcting the bias of the residuals, using a different set of adjustment matrices that we term CR2f. 
The CR2f estimator uses $f_{2f} = 1$ and
$$
\bmat{E}^\sb{2f}_j = \half{\bmat{W}}_j \bmat{A}^\sb{2f}_j \half{\bmat{W}}_j \bmat{X}_j
(\#eq:CR2f-Ej)
$$
inside \@ref(eq:V-CR) for $j=1,...,J$, where $\bmat{A}^\sb{2f}_1,...,\bmat{A}^\sb{2f}_j$ are adjustment matrices that satisfy the criteria
$$
\E\left(\bmat{A}^\sb{2f}_j\half{\bmat{W}}_j\bmat{\hat{u}}_j \bmat{\hat{u}}_j'\half{\bmat{W}}_j {\bmat{A}^\sb{2f}_j}'\right) = \bmat{A}^\sb{2f}_j \half{\bmat{W}}_j \bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j' \half{\bmat{W}}_j {\bmat{A}^\sb{2f}_j}' \\ = \half{\bmat{W}}_j \bs\Psi_j\half{\bmat{W}}_j
(\#eq:CR2f-criterion)
$$
for $j = 1,...,J$. Following the same reasoning as with the CR2 estimator, adjustment matrices that satisfy these criteria lead to a variance estimator that is exactly unbiased if the working model is correctly specified. 
A symmetric solution is given by 
$$
\bmat{A}^\sb{2f}_j = \half{\bmat{W}}_j {\bs\Psi^C_j}' \halfinv{\left(\bmat{B}_j^\sb{2f}\right)} \bs\Psi^C_j \half{\bmat{W}}_j,
(\#eq:CR2f-symm)
$$
where $\bs\Psi^C_j$ is again the upper-right triangular Cholesky factorization of $\bs\Psi_j$ and   
$$
\bmat{B}^\sb{2f}_j = \bs\Psi^C_j \bmat{W}_j \bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j' \bmat{W}_j {\bs\Psi^C_j}'.
(\#eq:CR2f-symm-Bj)
$$
@mccaffrey2001generalizations also give an asymmetric solution, but it is not recommended over the symmetric solution for reasons similar to those regarding the CR2 adjustments. 

If the weight matrices are inverse-variance, then the CR2f adjustment matrices have the simpler form 
$$
\bmat{A}^\sb{2f}_j = \halfinv{\left(\bmat{I}_j - \half{\bmat{W}}_j\bmat{X}_j \bmat{M} \bmat{X}_j' \half{\bmat{W}}_j\right)}.
(\#eq:CR2f-inv)
$$
For ordinary (unweighted) least squares, the CR2 and CR2f adjustment matrices are equivalent. 

The computational trick described by @niccodemi2020refining for the CR2 adjustment under ordinary least squares with an independence working model can be extended to the CR2f adjustment matrices as long as the weight matrices are inverse variance. 

<!-- H: Why don't we explain the "trick" of Niccodemi et al. (2020)? The gist of it would be of course its equation (6), but let's not go there... Instead, let's explain part of the followings. -->

1) We can set $\bmat{X}^*_j = \bmat{W}^C_j \bmat{X}_j$ (and $\bmat{Y}^*_j = \bmat{W}^C_j \bmat{Y}_j$) following @mccaffrey2001generalizations ($\bmat{X} = \bmat{W}^{1/2} \bmat{X}$) when $\bmat{W}_j = \bs\Psi_j^{-1}$ for WLS cases using CR2f.
2) Then, @niccodemi2020refining shows that \@ref(eq:V-CR) can be presented as $\bmat{V}^{CR} = (\bmat{X}^{*'}_j \bmat{X}^*_j)^{-1} \sum_{j=1}^J \left(\bmat{\tilde{s}}_j \bmat{\tilde{s}}_j^{'}\right) (\bmat{X}^{*'}_j \bmat{X}^*_j)^{-1}$ with $\bs{f}_{2f} = 1$, where $\bmat{\tilde{s}}_j = \bmat{X}^*_j \bmat{H}_j^{*-1/2} \bmat{\hat{u}}_j$, where $\bmat{H}_j^{*} = (\bmat{I}_p - \bmat{R}^{*'}_j \bmat{R}^*_j)$ and $\bmat{R}^*_j = \bmat{X}^*_j (\bmat{X}^{*'} \bmat{X}^*)^{-1/2}$. <- Any way to simplify/skip the $\bmat{H}_j$ and $\bmat{R}^*_j$?
3) If everything is plugged in and we keep with $\bmat{M} = \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bmat{X}_j\right)^{-1}$ (and $\bmat{M}^C = \bmat{M}^{1/2}$), \@ref(eq:CR2f-Ej) can be simplified to \ref(eq:ECR2f-quick).

Assuming that $\bmat{W}_j = \bs\Psi_j^{-1}$, then the CR2f estimating matrices are equal to 
$$
\bmat{E}^\sb{2f}_j = \bmat{W}_j \bmat{X}_j {\bmat{M}^{C}}'\halfinv{\left(\bmat{I}_p - \bmat{M}^C \bmat{X}_j'\bmat{W}_j \bmat{X}_j{\bmat{M}^C}' \right)} {\bmat{M}^{-C}}'.
(\#eq:ECR2f-quick)
$$

<!-- H: Does the first term on the right hand side start with $\bmat{W}_j$ instead of $\bmat{W}_j^C$? Also, see other questions in my note. -->

The advantage of using this form is that it only requires taking the inverse root of a $p \times p$ matrix, whereas calculating the adjustment matrix $\bmat{A}^\sb{2f}_j$ requires taking the inverse root of an $n_j \times n_j$ matrix. 
In applications involving large clusters with $n_j >> p$, using \@ref(eq:ECR2f-quick) will be much more efficient in terms of memory and computational burden. 

### CR3J, CR3, CR3$\lambda$

The cluster-level jackknife technique is closely related to cluster-robust variance estimation. A cluster-level jackknife sampling variance is given by
$$
\bmat{V}^{CR3J} = \frac{J - 1}{J} \sum_{j=1}^J \left(\bs{\hat\beta}_{(j)} - \bs{\hat\beta}\right) \left(\bs{\hat\beta}_{(j)} - \bs{\hat\beta}\right)',
(\#eq:jackknife)
$$
where $\bs{\hat\beta}_{(j)}$ is the weighted least squares estimator calculated while excluding the observations from cluster $j$.[^Other-jackknife] 
Although not immediately apparent, $\bmat{V}^{CR3J}$ is equivalent to calculating the cluster-robust variance estimator as given in \@ref(eq:V-CR) using $f_{3J} = \frac{J - 1}{J}$ and 
$$
\bmat{E}^\sb{3}_j = {\bmat{A}_j^\sb{3}}' \bmat{W}_j \bmat{X}_j,
(\#eq:jackknife-Ej)
$$
where 
$$
\bmat{A}_j^\sb{3} = \left(\bmat{I}_j - \bmat{X}_j \bmat{M} \bmat{X}_j' \bmat{W}_j\right)^{-1}.
(\#eq:jackknife-Aj)
$$
Sometimes, the leading factor in \@ref(eq:jackknife) is omitted, which yields the CR3 estimator. 
This is equivalent to taking $f_3 = 1$ and $\bmat{E}_j^\sb{3} = \bmat{E}_j^\sb{3J}$. 

[^Other-jackknife]: Other versions of the cluster-level jackknife might center the the deviations on the arithmetic mean of $\bs{\hat\beta}_{(1)},...,\bs{\hat\beta}_{(J)}$.

For the special case of unweighted least squares with an independent, homoskedastic working model, @niccodemi2020refining showed that the CR3 estimator has an upward bias and the CR3J estimator has an upward bias except when the predictors are identically distributed in each cluster, so that $\bmat{X}_j'\bmat{X}_j = \bmat{X}'\bmat{X} / J$. 
To reduce this bias, they suggested using $\bmat{E}_j^\sb{3}$ with the leading constant $f_{3\lambda} = \left(\sum_{j=1}^J \frac{n_j}{N - n_j}\right)^{-1}$, which becomes equivalent to CR3 when all clusters are balanced. 
Compared to CR3J, this CR3$\lambda$ estimator better accounts for variation in cluster sizes. 
For the case of unweighted least squares with the independent and homoskedastic working model, it will be approximately unbiased under the more general condition that $\bmat{X}_j'\bmat{X}_j = \frac{n_j}{N}\bmat{X}'\bmat{X}$. 

@niccodemi2020refining also described an efficient method of calculating the $\bmat{E}^\sb{3}_j$ matrices for the special case of ordinary least squares with an independence working model. 
Their method can be extended to weighted least squares estimation as well, such that $\bmat{E}_j^\sb{3}$ can be calculated as 
$$
\bmat{E}_j^\sb{3} = \bmat{W}_j \bmat{X}_j \left(\bmat{X}' \bmat{W} \bmat{X} - \bmat{X}_j' \bmat{W}_j \bmat{X}_j \right)^{-1}.
(\#eq:ECR3-quick)
$$
This representation avoids the need to invert $n_j \times n_j$ matrices, instead requiring only the inverse of $p \times p$ matrices. 
When $p$ is limited but $n_j$ is large, this can result in substantial computational savings [@niccodemi2020refining; see also @mackinnon2022fast]. 

<!-- Jackknife - See Tipton (2015) for some arguments -->
"Bell and McCaffrey (2002) showed that when W = I, the [CR0] estimator tends to underestimate the true variances, whereas the [CRJ] estimator tends to overestimate these variances; it is not difficult to show that these results extend to the $W \neq I$ case."
Simulation studies comparing cluster-robust variance estimators (HTJ, JK, MBB) and degrees of freedom adjustments for meta-analysis scenarios <- How relevant? How much of the results should we introduce? <- Describe only when the results cover WLS cases with $W \neq I$.

### CR4

The CR2 correction can be difficult to implement because it involves computing $n_j \times n_j$ correction matrices for each cluster. 
Some or all clusters include a large number of observations, then the matrices will be large and computing them may be quite time consuming. 
The computational methods described by @niccodemi2020refining and @mackinnon2022fast provide a way to reduce computation time of the CR2 and CR2f corrections under certain working models, but they are not entirely general. 
Here we describe a novel generalization of the CR2f adjustment, termed CR4, that can be applied to models estimated by weighted least squares, with an arbitrary working model and weights that are not necessarily inverse-variance. 

Like the CR2 and CR2f corrections, CR4 is designed to provide a variance estimator that is exactly unbiased when the working model is correctly specified. 
The main innovation is to do so by correcting the full score contribution $\bmat{X}_j' \bmat{W}_j \bmat{\hat{u}}_j$ rather than correcting the residuals, so that the adjustment matrices $\bmat{A}^\sb{4}_j$ are of dimension $p \times p$ rather than $n_j \times n_j$. 

The CR4 variance estimator uses $f_4 = 1$ and estimating matrices given by
$$
\bmat{E}^\sb{4}_j = \bmat{W}_j \bmat{X}_j \bmat{A}^\sb{4}_j,
(\#eq:CR4-Ej)
$$
where $\bmat{A}^\sb{4}_j$ are adjustment matrices chosen to satisfy the criterion
$$
\E\left({\bmat{E}^\sb{4}_j}' \bmat{\hat{u}}_j \bmat{\hat{u}}_j' \bmat{E}^\sb{4}_j \right) = \bmat{X}_j' \bmat{W}_j \bs\Psi_j \bmat{W}_j \bmat{X}_j
(\#eq:CR4-criterion)
$$
for $j = 1,...,J$. 

To construct the CR4 adjustment matrices, we first define the matrices $\bmat{F}_j = \bmat{X}_j' \bmat{W}_j \bs\Psi_j \bmat{W}_j \bmat{X}_j$ and $\bmat{G}_j = \bmat{X}_j' \bmat{W}_j \bmat{X}_j$. 
With these terms, the model-based variance estimator defined in \@ref(eq:V-model) can be computed as $\bmat{V}^M = \bmat{M}\left(\sum_{j=1}^J \bmat{F}_j \right)\bmat{M}$, but substituting the estimated working model $\bs{\hat\Psi}_j$ in place of the known working model $\bs\Psi_j$ in $\bmat{F}_1,...,\bmat{F}_J$. 

The CR4 adjustment matrices are then 
$$
\bmat{A}^\sb{4}_j = {\bmat{F}_j^C}' \halfinv{\left(\bmat{B}_j^\sb{4}\right)} \bmat{F}_j^C,
(\#eq:CR4-Aj)
$$
where $\bmat{F}_j^C$ is the upper-right Cholesky factorization of $\bmat{F}_j$ and 
$$
\bmat{B}_j^\sb{4} = \bmat{F}_j^C \left(\bmat{F}_j - \bmat{G}_j \bmat{M}\bmat{F}_j - \bmat{F}_j \bmat{M}\bmat{G}_j + \bmat{G}_j \bmat{V}^M \bmat{G}_j\right) {\bmat{F}_j^C}'.
(\#eq:CR4-Bj)
$$
To see that these adjustment matrices satisfy the criterion given in \@ref(eq:CR4-criterion), observe that the expectation of the cross-product of adjusted scores can be written as 
$$
\begin{aligned}
\E\left(\bmat{A}_j^\sb{4}\bmat{X}_j' \bmat{W}_j \bmat{\hat{u}}_j \bmat{\hat{u}}_j' \bmat{W}_j \bmat{X}_j \bmat{A}_j^\sb{4}\right) &= \bmat{A}_j^\sb{4} \bmat{X}_j' \bmat{W}_j\bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j'\bmat{W}_j \bmat{X}_j \bmat{A}_j^\sb{4} \\
&= \bmat{A}_j^\sb{4} \left(\bmat{F}_j - \bmat{G}_j \bmat{M} \bmat{F}_j - \bmat{F}_j \bmat{M} \bmat{G}_j + \bmat{G}_j \bmat{V}^M \bmat{G}_j \right) \bmat{A}_j^\sb{4} \\
&= {\bmat{F}_j^C}' \halfinv{\left(\bmat{B}_j^\sb{4}\right)} \bmat{F}_j^C \left(\bmat{F}_j - \bmat{G}_j \bmat{M} \bmat{F}_j - \bmat{F}_j \bmat{M} \bmat{G}_j + \bmat{G}_j \bmat{V}^M \bmat{G}_j \right){\bmat{F}_j^C}' \halfinv{\left(\bmat{B}_j^\sb{4}\right)} \bmat{F}_j^C \\
&= {\bmat{F}_j^C}' \halfinv{\left(\bmat{B}_j^\sb{4}\right)} \bmat{B}_j^\sb{4} \halfinv{\left(\bmat{B}_j^\sb{4}\right)} \bmat{F}_j^C \\
&= \bmat{F}_j.
\end{aligned}
(\#eq:CR4-crit-sat)
$$
It follows that using the CR4 adjustment matrices leads to an exactly unbiased variance estimator when the working model is correct, just like the CR2 and CR2f corrections. 

If the model is estimated using inverse-variance weights so that $\bmat{W}_j = \bs\Psi_j^{-1}$, then $\bmat{F}_j = \bmat{G}_j$ and the CR4 adjustment matrices simplify to 
$$
\bmat{A}^\sb{4}_j = {\bmat{G}_j^C}' \halfinv{\left(\bmat{G}_j^C \left(\bmat{G}_j - \bmat{G}_j \bmat{M} \bmat{G}_j\right) {\bmat{G}_j^C}'\right)} \bmat{G}_j^C.
(\#eq:CR4-inv)
$$
<!-- Can this be simplified further? H: I think this section is not at all too complicated currently. -->

<!-- How is CR4 related to CR2f? Are they equivalent for inverse-variance weights? -->
<!-- How is CR2 related to CR4 for OLS with $\psi = \sigma^2 I$? -->

A further benefit of the CR4 adjustment matrices is that they can be tailored to a subset of the covariates that are of focal interest. 
Rather than correcting the bias in the full variance estimator, we could correct the bias for just one or more of the predictors, so that $\E\left(\bmat{k}' \bmat{V}^{CR} \bmat{k}\right) = \Var\left(\bmat{k}' \bs{\hat\beta}\right)$ for a $p \times k$ matrix of contrasts $\bmat{k}$ with $k \leq p$, even if unbiasedness does not hold for the full estimator $\bmat{V}^{CR}$. 

<!-- H: Can we explain the above more specifically? Would this be related to the fact that CR4 corrects the full score distribution instead of the residuals? -->

## Fixed effects models

## Instrumental variables models


# Model-specific considerations

## `lm()`

## `mlm()`

## `nlme::gls()`, `nlme::lme()`, and `lme4::lmer()`

## `glm()` and `geepack::geeglm()`

## `robumeta::robu()`

## `metafor::rma.uni()` and `metafor::rma.mv()`

## `plm::plm()`

## `ivreg::ivreg()`

# Single-coefficient hypothesis tests and confidence intervals {#sec:t-test}

<!-- (To use the Satterthwaite approximation (instead of the Wald-test assuming large samples), we need the mean and variance of $V$. These are generally unknown (because they depend on $\bs\Omega$), but we can approximate them by assuming that the working model is correct. - H: I think this is the key idea/motivation behind cluster-robust variance estimation. -->

## Degrees of freedom corrections

- z
- naive-t, naive-tp
- Satterthwaite correction

## Saddlepoint correction

## `coef_test()`

## `conf_int()`

## `linear_contrast()`


# Multiple-constraint hypothesis tests

- `Wald_test()`

## Convenience functions for constraint matrices

- `constrain_equal()`
- `constrain_zero()`
- `constrain_pairwise()`


# Computational considerations


# References {-}
