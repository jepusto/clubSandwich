---
title: "Cluster-robust variance estimation with clubSandwich"
author: "James E. Pustejovsky and Hanna Kim"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    number_sections: false
bibliography: bibliography.bib
vignette: >
  %\VignetteIndexEntry{Cluster-robust variance estimation with clubSandwich}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

$$
\newcommand{\Prob}{\text{Pr}}
\newcommand{\E}{\text{E}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\corr}{\text{corr}}
\newcommand{\Var}{\text{Var}}
\newcommand{\tr}{\text{tr}}
\newcommand{\bmat}[1]{\mathbf{#1}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\half}[1]{#1^{\frac{1}{2}}}
\newcommand{\halfinv}[1]{#1^{-\frac{1}{2}}}
\newcommand{\sb}[1]{{\{#1\}}}
$$

Linear regression models, estimated by ordinary least squares weighted least squares, are a ubiquitous tool across a diverse range of fields. 
Classically, inference in such models is based on the assumption that the model's errors are independent and homoskedastic (or more generally, that the errors follow a known structure of a low-dimensional parameter). 
However, such assumptions will be unreasonable in many applications. 
For instance, using the classical assumptions can lead to underestimated standard errors when observations are drawn from groups or clusters that are subject to shared sources of error.
Cluster-robust variance estimation methods provide a basis for inference under the weaker assumption that the errors of observations from different clusters are independent but errors within a given cluster may be correlated. 
These methods can be applied across a broad scope of models, including not only ordinary linear regression, but also hierarchical (random effects) linear models, meta-analytic models, generalized linear models, generalized estimating equations, and instrumental variables models. 

Let us consider the linear regression model
$$
\begin{aligned}
y_{ij} &= \beta_0 + \beta_1 x_{1ij} + \beta_2 x_{2ij} + \cdots + \beta_{p-1} x_{p-1,ij} + e_{ij}, \\
 &= \mathbf{x}_{ij} \boldsymbol\beta + e_{ij}
\end{aligned}
(\#eq:reg-model)
$$
where $\mathbf{x}_{ij}$ is a $1 \times p$ row vector of predictors (possibly including an intercept term) and $e_{ij}$ is an error term, both for observation $i = 1,...,n_j$ in cluster $j = 1,...,J$. It will be convenient to write \@ref(eq:reg-model) using vectors and matrices for each cluster, as
$$
\mathbf{y}_j = \mathbf{X}_j \boldsymbol\beta + \mathbf{e}_j,
(\#eq:reg-matrix)
$$
where $\mathbf{y}_j = (y_{1j} \cdots y_{n_j j})'$ is the $n_j \times 1$ vector of outcomes for cluster $j$, $\mathbf{X}_j = (\mathbf{x}_{1j}' \cdots \mathbf{x}_{n_j j}')'$ is the $n_j \times p$ matrix of predictors for cluster $j$, and $\mathbf{e}_j$ is the $n_j \times 1$ vector of errors for cluster $j$, all for $j = 1,...,J$. 

We will maintain the following assumptions regarding the errors in the regression:
$$
\begin{aligned}
A1:& \qquad \E(\bmat{e}_j | \bmat{X}_j) = \bmat{0} \\ 
A2:& \qquad \E(\bmat{e}_j \bmat{e}_{k}' | \bmat{X}_j, \bmat{X}_{k}) = \bmat{0}, \qquad \text{when} \qquad j \neq {k} \\
A3:& \qquad \E(\bmat{e}_j \bmat{e}_j' | \bmat{X}_j) = \Var(\bmat{e}_j) = \bs\Omega_j.
\end{aligned}
(\#eq:reg-assumptions)
$$
The $n_j \times n_j$ matrices $\bs\Omega_j$ capture the cluster-specific variance-covariance matrix of the errors. 
A common strategy for estimation and inference about the regression coefficients is to make further assumptions regarding the form of these matrices, such as by assuming that all errors are independent and homoskedastic (i.e., $\bs\Omega_j = \sigma^2 \bmat{I}_j$) or that they have a structure characterized by a low-dimensional parameter (as in a hierarchical linear model or random effects model). 
Robust variance estimation methods provide a method for characterizing uncertainty and conducting inference without relying on such additional assumptions. 
Rather, we will treat the variance-covariance matrices as following an unknown form. 
Thus, the errors within a cluster need not be homoskedastic or independent, but can instead be arbitrarily correlated within the same cluster. 

Researchers will often estimate the regression coefficient vector $\bs\beta$ using ordinary least squares (OLS) or, more generally, some form of weighted least squares (WLS). 
The weighted least squares estimator might involve non-unit weights for each observation or some form of weight matrices for each cluster, potentially including off-diagonal terms (as in a hierarchical linear model). 
We will consider a general set of symmetric weight matrices $\bmat{W}_1,...,\bmat{W}_J$. 
The weighted least squares estimator for $\bs\beta$ is then defined as
$$
\bs{\hat\beta} = \bmat{M} \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bmat{y}_j\right), \quad \text{where} \quad \bmat{M} = \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bmat{X}_j\right)^{-1}.
(\#eq:WLS-M)
$$
In some applications, the weight matrices used for WLS estimation are motivated by a working model for the structure of the error variance-covariances $\bs\Omega_j$. 
In this approach, the analyst posits a structure $\bs\Omega_j = \bs\Psi_j(\bs\theta)$ for some low-dimensional, estimable parameter $\bs\theta$. 
If the working model captures the true form of the error variances, then the weights that make the sampling variance of $\bs{\hat\beta}$ as small as possible (the most precise) are the inverse of the variance-covariance matrix of the errors, which depend on the parameter $\bs\theta$. 
In practice, the variance parameter $\bs\theta$ is unknown and so the analyst will use the inverse of the _estimated_ working model as the weights, with $\bmat{W}_j = \bs{\hat\Psi}_j^{-1} = \bs\Psi_j^{-1}(\bs{\hat\theta})$. 
When the weight matrices are obtained following this strategy, we will refer to them as "inverse-variance" weights. 

# Robust Variance Estimation

Treating the weight matrices as fixed, the sampling variance of $\bs{\hat\beta}$ is 
$$
\Var\left(\bs{\hat\beta}\right) = \bmat{M} \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bs\Omega_j \bmat{W}_j \bmat{X}_j \right) \bmat{M},
(\#eq:var-beta-hat)
$$
which depends on the unknown variance-covariances $\bs\Omega_1,...,\bs\Omega_J$. 
One strategy for estimating \@ref(eq:var-beta-hat) is to substitute an estimated working model $\bs{\hat\Psi}_j$ for the unknown variance-covariance matrices $\bs\Omega_j$, giving a model-based variance estimator:
$$
\bmat{V}^M = \bmat{M} \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bs{\hat\Psi}_j \bmat{W}_j \bmat{X}_j \right) \bmat{M}.
(\#eq:V-model)
$$
If the working model is correctly specified and precisely estimated, then the model-based variance estimator will closely approximate the true sampling variance. 
If the analyst uses inverse-variance weights based on the same working model, then the model-based variance estimator has the simpler form $\bmat{V}^{model} = \bmat{M}$. 
However, if the working model is inaccurate or incorrectly specified in some way, then there is no guarantee that the model-based variance estimator will be valid. 

The original form of the robust variance estimator uses the residuals from each cluster to derive rough estimates of the unknown error variance matrices. 

<!-- H: Do we mean the linear regression residuals when we estimate (2)? The hat(u_j) notation appears here for the first time. If we instead refer to the same hat(u_j)'s as in the CR2 notation, should we bring up the definition C_j(I_N - H)? -->

This estimator substitutes the cross product of the residuals $\bmat{\hat{u}}_j\bmat{\hat{u}}_j'$ in place of $\bs\Omega_j$ in \@ref(eq:var-beta-hat), leading to the variance estimator
$$
\bmat{V}^{CR0} = \bmat{M} \left(\sum_{j=1}^J \bmat{X}_j'\bmat{W}_j \bmat{\hat{u}}_j \bmat{\hat{u}}_j' \bmat{W}_j\bmat{X}_j\right) \bmat{M}.
(\#eq:V-CR0)
$$
The cross-product of the residuals from a given cluster is a very crude estimator for the error variance matrix from that cluster. 
However, we do not need to estimate each cluster-specific variance matrix accurately. 
Rather, we only need to approximate the _sum_ of the terms involving the error variance matrix, and even the crude CR0 estimator works for this purpose if the number of independent clusters is sufficiently large. 
However, it is often the case that neither the number of clusters nor the observations per cluster is large enough, such that \@ref(eq:V-CR0) underestimates the sampling variance of $\bs{\hat\beta}$. It should be noted that the number of clusters deemed as "few" is not clear-cut and depends on specific study conditions, requiring more clusters when the clusters are unbalanced. [@cameron2015practitioner]. 

<!-- H: I don't include the over-rejection (narrow CI) problem here. -->


## Small-sample corrections

A number of variations on the basic cluster-robust variance estimator have been proposed, all of which aim to provide more accurate approximations when the number of clusters is limited. 
The variations implemented in the `clubSandwich` package all have the form
$$
\bmat{V}^{CR} = f \times \bmat{M} \left(\sum_{j=1}^J \bmat{E}_j' \bmat{\hat{u}}_j \bmat{\hat{u}}_j' \bmat{E}_j \right) \bmat{M}
(\#eq:V-CR)
$$
for some constant $f$ and some estimating matrices $\bmat{E}_1,...,\bmat{E}_J$. 
With this representation, the CRO estimator corresponds to $f_0 = 1$ and $\bmat{E}^\sb{0}_j = \bmat{W}_j \bmat{X}_j$. 


### CR1

The CR1, CR1S, and CR1p estimators are variants that use the following constants with $\bmat{E}^\sb{1}_j = \bmat{W}_j \bmat{X}_j$. 
$$
\begin{aligned}
\text{CR1}: f_1 &= \frac{J}{J - 1}, \\
\text{CR1S}: f_{1S} &= \frac{J (N - 1)}{(J - 1)(N - p)} , \\
\text{CR1p}: f_{1p} &= \frac{J}{J - p}, 
\end{aligned}
(\#eq:CR1)
$$
These corrections are all motivated by the observation that the sandwich estimator corresponds to a sample variance-covariance matrix across terms for each cluster. 
Thus, inflating it by an amount depending on the number of clusters should tend to reduce the small-sample bias of CR0. 
However, the exact bias of CR0 depends on features of the data beyond simply the number of clusters and predictors, including cluster sizes and the distribution of the predictors within and across clusters [@mccaffrey2001generalizations;@tipton2015small;@pustejovsky2018small]. 
Thus, the simple multiplicative corrections involved in the CR1 and related estimators are only very rough approximations, and do not provide perfect finite-cluster corrections [@cameron2015practitioner]. 


### CR2

@bell2002bias and @mccaffrey2001generalizations introduced a class of more refined robust variance estimators, termed "bias-reduced linearization" estimators, that account for variation in cluster sizes and the distribution of predictors. 
These estimators require positing a working model for the error structure, based on which the estimators incorporate adjustments to the residuals used in the middle of the sandwich estimator. 
The whole estimator is exactly unbiased when the working model is correctly specified. 
Although in principle a working model is required prior to the whole process, we rarely have a known working model for the error structure of regression coefficients. Therefore, in practice, we usually substitute residuals from estimated models, analogous to using frequentist model estimates as convenient priors for Bayesian estimation. 

<!-- Added sentences to clarify the concept of the (known) working model behind CR2, even though in practice we choose to use the estimated working model ~ priors in Bayesian. -->

The CR2 estimator uses $f_2 = 1$ and
$$
\bmat{E}^\sb{2}_j = {\bmat{A}^\sb{2}_j}' \bmat{W}_j \bmat{X}_j,
(\#eq:CR2-Ej)
$$
for $j=1,...,J$, where $\bmat{A}^\sb{2}_1,...,\bmat{A}^\sb{2}_J$ are adjustment matrices that satisfy the criteria
$$
\E\left(\bmat{A}^\sb{2}_j\bmat{\hat{u}}_j \bmat{\hat{u}}_j' {\bmat{A}^\sb{2}_j}'\right) = \bs\Psi_j,
(\#eq:CR2-criterion)
$$
where $\bs\Psi_j$ denotes the working model for cluster $j$ and the expectation is taken assuming that the working model is correct. 
Thus, if the working model is correctly specified, plugging in \@ref(eq:CR2-Ej) to \@ref(eq:V-CR) leads to
$$
\E\left({\bmat{E}^\sb{2}_j}'\bmat{\hat{u}}_j \bmat{\hat{u}}_j' \bmat{E}^\sb{2}_j\right) = \bmat{X}_j'\bmat{W}_j \bs\Psi_j \bmat{W}_j \bmat{X}_j,
(\#eq:CR2-expct)
$$
and so $\E\left(\bmat{V}^{CR2}\right) = \Var\left(\bs{\hat\beta}\right)$. 

To derive expressions for $\bmat{A}^\sb{2}_j$, observe that the residuals from cluster $j$ may be written as $\bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bmat{Y}$, where $\bmat{H} = \bmat{X}\bmat{M}\bmat{X}'\bmat{W}$ and $\bmat{C}_j$ is an $n_j \times N$ matrix that selects the rows of the full data that correspond to cluster $j$ (i.e., $\bmat{X}_j = \bmat{C}_j \bmat{X}$). Because the residuals have mean zero, the expectation of the cross-product of residuals from cluster $j$ is
$$
\E\left(\bmat{\hat{u}}_j \bmat{\hat{u}}_j'\right) = \bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j'.
(\#eq:resid-expr)
$$
The adjustment matrices therefore satisfy the criteria
$$
\bmat{A}^\sb{2}_j \bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j' {\bmat{A}^\sb{2}_j}' = \bs\Psi_j
(\#eq:CR2-crit-sat)
$$
for $j = 1,...,J$. 

There are multiple possible solutions to the above. @mccaffrey2001generalizations considered several possible solutions to the above, including a symmetric solution given by 
$$
\bmat{A}^\sb{2}_j = {\bs\Psi^C_j}' \halfinv{\left(\bmat{B}_j^\sb{2}\right)} \bs\Psi^C_j,
(\#eq:CR2-symm)
$$
<!-- H: Check notation for the B_j matrix inside. -->

where $\bs\Psi^C_j$ is the upper-right triangular Cholesky factorization of $\bs\Psi_j$, so that ${\bs\Psi^C_j}' \bs\Psi^C_j = \bs\Psi_j$, 
$$
\bmat{B}^\sb{2}_j = \bs\Psi^C_j \bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j' {\bs\Psi^C_j}',
(\#eq:CR2-symm-Bj)
$$
and $\halfinv{\left(\bmat{B}_j^\sb{2}\right)}$ is the inverse of the symmetric square root of $\bmat{B}^\sb{2}_j$.

If the weight matrices are inverse-variance, then the matrix $\bmat{B}^\sb{2}_j$ has the simpler form
$$
\bmat{B}^\sb{2}_j = \bs\Psi^C_j \left(\bs\Psi_j - \bmat{X}_j\bmat{M}\bmat{X}_j'\right) {\bs\Psi^C_j}'.
(\#eq:CR2-inv-Bj)
$$

For the special case of ordinary least squares, the adjustment matrices simplify even further to
$$
\bmat{A}^\sb{2}_j = \halfinv{\left(\bmat{I}_j - \bmat{X}_j \bmat{M}\bmat{X}_j'\right)}
(\#eq:CR2-OLS)
$$
if the working model is based on the assumption of independent, homoskedastic errors. 

@mccaffrey2001generalizations also considered an asymmetric solution to the criteria. However, they recommended the symmetric one because it minimizes the squared error between the OLS residuals and the true errors when they are independent and homoskedastic. Even when the errors were not independent, @mccaffrey2001generalizations found that the symmetric adjustment matrices yielded smaller bias than asymmetric ones.

@niccodemi2020refining described an efficient method of calculating the CR2 for the special case of ordinary least squares with an independence working model...
Unfortunately, their computational approach does not generalize to the CR2 adjustment matrices for weighted least squares or working models other than $\Psi_j = \sigma^2 \bmat{I}_j$. 


### CR2f

@mccaffrey2001generalizations also considered another approach to correcting the bias of the residuals, using a different set of adjustment matrices that we term CR2f. The CR2f estimator uses $f_2 = 1$ and
$$
\bmat{E}^\sb{2f}_j = \half{\bmat{W}}_j \bmat{A}^\sb{2f}_j \half{\bmat{W}}_j \bmat{X}_j,
(\#eq:CR2f-Ej)
$$
where $\bmat{A}^\sb{2f}_1,...,\bmat{A}^\sb{2f}_j$ are adjustment matrices that satisfy the criteria
$$
\E\left(\bmat{A}^\sb{2f}_j\half{\bmat{W}}_j\bmat{\hat{u}}_j \bmat{\hat{u}}_j'\half{\bmat{W}}_j {\bmat{A}^\sb{2f}_j}'\right) = \bmat{A}^\sb{2f}_j \half{\bmat{W}}_j \bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j' \half{\bmat{W}}_j {\bmat{A}^\sb{2f}_j}' \\ = \half{\bmat{W}}_j \bs\Psi_j\half{\bmat{W}}_j.
(\#eq:CR2f-criterion)
$$
Following the same reasoning as with the CR2 estimator, adjustment matrices that satisfy these criteria lead to a variance estimator that is exactly unbiased if the working model is correctly specified. A symmetric solution is given by 
$$
\bmat{A}^\sb{2f}_j = \half{\bmat{W}}_j {\bs\Psi^C_j}' \halfinv{\left(\bmat{B}_j^\sb{2f}\right)} \bs\Psi^C_j \half{\bmat{W}}_j,
(\#eq:CR2f-symm)
$$
where $\bs\Psi^C_j$ is again the upper-right triangular Cholesky factorization of $\bs\Psi_j$ and   
$$
\bmat{B}^\sb{2f}_j = \bs\Psi^C_j \bmat{W}_j \bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j' \bmat{W}_j {\bs\Psi^C_j}'.
(\#eq:CR2f-symm-Bj)
$$
In a similar way to the CR2 estimator, an asymmetric solution is given by 
@mccaffrey2001generalizations, using which is not recommended over the symmetric solution.

If the weight matrices are inverse-variance, then the CR2f adjustment matrices have the simpler form
$$
\bmat{A}^\sb{2f}_j = \halfinv{\left(\bmat{I}_j - \half{\bmat{W}}_j\bmat{X}_j \bmat{M} \bmat{X}_j' \half{\bmat{W}}_j\right)}.
(\#eq:CR2f-inv)
$$
For ordinary (unweighted) least squares, the CR2 and CR2f adjustment matrices are equivalent.

### CRJ, CR3, CR3$\lambda$

The cluster-level jackknife technique is closely related to cluster-robust variance estimation. A cluster-level jack-knife sampling variance is given by
$$
\bmat{V}^{CRJ} = \frac{J - 1}{J} \sum_{j=1}^J \left(\bs{\hat\beta}_{(j)} - \bs{\hat\beta}\right) \left(\bs{\hat\beta}_{(j)} - \bs{\hat\beta}\right)',
(\#eq:jackknife)
$$
where $\bs{\hat\beta}_{(j)}$ is the weighted least squares estimator calculated while excluding the observations from cluster $j$.[^Other-jackknife] Although not immediately apparent, $\bmat{V}^{CRJ}$ is equivalent to calculating the cluster-robust variance estimator as given in (Equation) using $f_J = \frac{J - 1}{J}$ and 
$$
\bmat{E}^\sb{J}_j = {\bmat{A}_j^\sb{3}}' \bmat{W}_j \bmat{X}_j,
(\#eq:jackknife-Ej)
$$
where 
$$
\bmat{A}_j^\sb{3} = \left(\bmat{I}_j - \bmat{X}_j \bmat{M} \bmat{X}_j' \bmat{W}_j\right)^{-1}.
(\#eq:jackknife-Aj)
$$
Sometimes, the leading factor in \@ref(eq:jackknife) is omitted, which yields the CR3 estimator. This is equivalent to taking $f_3 = 1$ and $\bmat{E}_j^\sb{3} = \bmat{E}_j^\sb{J}$. 

For the special case of unweighted least squares with an independent, homoskedastic working model, @niccodemi2020refining showed that the CR3 estimator has an upward bias and the CRJ estimator has an upward bias except when the predictors are identically distributed in each cluster, so that $\bmat{X}_j'\bmat{X}_j = \bmat{X}'\bmat{X} / J$. To reduce this bias, they suggested using $\bmat{E}_j^\sb{3}$ with the leading constant $f_{3\lambda} = \left(\sum_{j=1}^J \frac{n_j}{N - n_j}\right)^{-1}$. Compared to CRJ, this CR3$\lambda$ estimator better accounts for variation in cluster sizes. For the case of unweighted least squares with the independent and homoskedastic working model, it will be unbiased under the more general condition that $\bmat{X}_j'\bmat{X}_j = \frac{n_j}{N}\bmat{X}'\bmat{X}$. 

@niccodemi2020refining also described an efficient method of calculating the $\bmat{E}^\sb{3}_j$ matrices for the special case of ordinary least squares with an independence working model...

[^Other-jackknife]: Other versions of the cluster-level jackknife might center the the deviations on the arithmetic mean of $\bs{\hat\beta}_{(1)},...,\bs{\hat\beta}_{(J)}$.

<!-- Jackknife - See Tipton (2015) for some arguments -->

### CR4 

The CR2, CR2f, and CR3 corrections can be difficult to implement because they involve computing $n_j \times n_j$ correction matrices for each cluster. Some or all clusters include a large number of observations, then the matrices will be large and computing them may be quite time consuming. The computational methods described by @niccodemi2020refining and @mackinnon2022fast provide a way to reduce computation time for certain specifications and certain working models, but they are not entirely general. Here we describe a novel generalization of the CR2f adjustment, termed CR4, that can be applied to models estimated by weighted least squares, with an arbitrary working model and weights that are not necessarily inverse-variance. 

Like the CR2 and CR2f corrections, CR4 is designed to provide a variance estimator that is exactly unbiased when the working model is correctly specified. The main innovation is to do so by correcting the full score contribution $\bmat{u}_j'\bmat{W}_j \bmat{X}_j$ rather than correcting the residuals, so that the adjustment matrices are of dimension $p \times p$ rather than $n_j \times n_j$. The CR4 variance estimator uses $f_4 = 1$ and estimating matrices given by
$$
\bmat{E}^\sb{4}_j = \bmat{W}_j \bmat{X}_j \bmat{A}^\sb{4}_j,
(\#eq:CR4-Ej)
$$
where $\bmat{A}^\sb{4}_j$ are adjustment matrices chosen to satisfy the criterion
$$
\E\left({\bmat{E}^\sb{4}_j}' \bmat{u}_j \bmat{u}_j' \bmat{E}^\sb{4}_j \right) = \bmat{X}_j' \bmat{W}_j \bs\Psi_j \bmat{W}_j \bmat{X}_j
(\#eq:CR4-criterion)
$$
for $j = 1,...,J$.

To construct the CR4 adjustment matrices, we first define the matrices $\bmat{F}_j = \bmat{X}_j' \bmat{W}_j \bs\Psi_j \bmat{W}_j \bmat{X}_j$ and $\bmat{G}_j = \bmat{X}_j' \bmat{W}_j \bmat{X}_j$. With these terms, the model-based variance estimator defined in \@ref(eq:V-model) can be computed as $\bmat{V}^M = \bmat{M}\left(\sum_{j=1}^J \bmat{F}_j \right)\bmat{M}$. The CR4 adjustment matrices are then
$$
\bmat{A}^\sb{4}_j = {\bmat{G}_j^C}' \halfinv{\left(\bmat{B}_j^\sb{4}\right)} \bmat{G}_j^C,
(\#eq:CR4-Aj)
$$
where $\bmat{G}_j^C$ is the upper-right Cholesky factorization of $\bmat{G}_j$ and 
$$
\bmat{B}_j^\sb{4} = \bmat{G}_j^C \left(\bmat{F}_j - \bmat{G}_j \bmat{M}\bmat{F}_j - \bmat{F}_j \bmat{M}\bmat{G}_j + \bmat{G}_j \bmat{V}^M \bmat{G}_j\right) {\bmat{G}_j^C}'.
(\#eq:CR4-Bj)
$$

To see that these adjustment matrices satisfy the criterion given in \@ref(eq:CR4-criterion), observe that the expectation of the cross-product of adjusted scores can be written as
$$
\begin{aligned}
\E\left(\bmat{A}_j^\sb{4}\bmat{X}_j' \bmat{W}_j \bmat{u}_j \bmat{u}_j' \bmat{W}_j \bmat{X}_j \bmat{A}_j^\sb{4}\right) &= \bmat{A}_j^\sb{4} \bmat{X}_j' \bmat{W}_j\bmat{C}_j\left(\bmat{I}_N - \bmat{H}\right) \bs\Psi \left(\bmat{I}_N - \bmat{H}'\right)\bmat{C}_j'\bmat{W}_j \bmat{X}_j \bmat{A}_j^\sb{4} \\
&= \bmat{A}_j^\sb{4} \left(\bmat{F}_j - \bmat{G}_j \bmat{F}_j - \bmat{F}_j \bmat{G}_j + \bmat{G}_j \bmat{V}^M \bmat{G}_j \right) \bmat{A}_j^\sb{4} \\
&= {\bmat{G}_j^C}' \halfinv{\left(\bmat{B}_j^\sb{4}\right)} \bmat{G}_j^C \left(\bmat{F}_j - \bmat{G}_j \bmat{F}_j - \bmat{F}_j \bmat{G}_j + \bmat{G}_j \bmat{V}^M \bmat{G}_j \right){\bmat{G}_j^C}' \halfinv{\left(\bmat{B}_j^\sb{4}\right)} \bmat{G}_j^C \\
&= {\bmat{G}_j^C}' \halfinv{\left(\bmat{B}_j^\sb{4}\right)} \bmat{B}_j^\sb{4} \halfinv{\left(\bmat{B}_j^\sb{4}\right)} \bmat{G}_j^C \\
&= \bmat{G}_j.
\end{aligned}
(\#eq:CR4-crit-sat)
$$
It follows that using the CR4 adjustment matrices leads to an exactly unbiased variance estimator when the working model is correct, just like the CR2 and CR2f corrections. 

If the model is estimated using inverse-variance weights so that $\bmat{W}_j = \bs\Psi_j^{-1}$, then the CR4 adjustment matrices simplify to
$$
\bmat{A}^\sb{4}_j = {\bmat{G}_j^C}' \halfinv{\left(\bmat{G}_j^C \left(\bmat{G}_j - \bmat{G}_j \bmat{M} \bmat{G}_j\right) {\bmat{G}_j^C}'\right)} \bmat{G}_j^C.
(\#eq:CR4-inv)
$$
<!-- Can this be simplified further? -->
<!-- How is CR4 related to CR2f? Are they equivalent for inverse-variance weights?-->

A further benefit of the CR4 adjustment matrices is that they can be tailored to a subset of the covariates that are of focal interest. Rather than correcting the bias in the full variance estimator, we could correct the bias for just one or more of the predictors, so that $\E\left(\bmat{k}' \bmat{V}^{CR} \bmat{k}\right) = \Var\left(\bmat{k}' \bs{\hat\beta}\right)$ for a $p \times k$ matrix of contrasts $\bmat{k}$ with $k \leq p$, even if unbiasedness does not hold for the full estimator $\bmat{V}^{CR}$.

# Model-specific considerations

## `lm()`

## `mlm()`

## `nlme::gls()`, `nlme::lme()`, and `lme4::lmer()`

## `plm::plm()`

## `glm()` and `geepack::geeglm()`

## `robumeta::robu()`

## `metafor::rma.uni()` and `metafor::rma.mv()`

# Single-coefficient hypothesis tests and confidence intervals

<!-- (To use the Satterthwaite approximation (instead of the Wald-test assuming large samples), we need the mean and variance of $V$. These are generally unknown (because they depend on $\bs\Omega$), but we can approximate them by assuming that the working model is correct. - H: I think this is the key idea/motivation behind cluster-robust variance estimation. -->

## Degrees of freedom corrections

- z
- naive-t, naive-tp
- Satterthwaite correction

## Saddlepoint correction

## `coef_test()`

## `conf_int()`

## `linear_contrast()`


# Multiple-constraint hypothesis tests

- `Wald_test()`

## Convenience functions for constraint matrices

- `constrain_equal()`
- `constrain_zero()`
- `constrain_pairwise()`

# Computational considerations

# References {-}
