---
title: "Corrigendum: Small sample methods for cluster-robust variance estimation and hypothesis testing in fixed effects models"
blinded: 0
authors:
  - name: James E. Pustejovsky
    affiliation: University of Wisconsin - Madison
    thanks: "Department of Educational Psychology, University of Wisconsin - Madison, 1025 West Johnson Street, Madison, WI 53706. Email: pustejovsky@wisc.edu"
  - name: Elizabeth Tipton
    affiliation: Northwestern University
    thanks: "Department of Statistics, Northwestern University. Email: tipton@northwestern.edu"
bibliography: bibliography.bib
biblio-style: agsm
keep_tex: yes
output:
  rticles::asa_article: 
    citation_package: default
    number_sections: false
abstract: |
  @pustejovsky2018small considered how to implement cluster-robust variance estimators for fixed effects models estimated by weighted (or unweighted) least squares. Theorem 2 of the paper concerns a computational short cut for a certain cluster-robust variance estimator in models with cluster-specific fixed effects. It claimed that this short cut works for models estimated by generalized least squares, as long as the weights are taken to be inverse of the working model. However, the theorem is incorrect. In this corrigendum, we  review the CR2 variance estimator, describe the assertion of the theorem as originally stated, and demonstrate the error with a counter-example. We then provide a revised version of the theorem, which holds for the more limited set of models estimated by ordinary least squares.

header-includes:
- \usepackage{amsthm}
- \newtheorem*{thm}{Theorem}
- \newtheorem{lem}{Lemma}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage[natbibapa]{apacite}
- \usepackage{caption}
- \usepackage{multirow}
- \usepackage{float}    % for fig.pos='H'
- \usepackage{rotfloat} % for sidewaysfigure
- \hypersetup{hidelinks}
- \newcommand{\Prob}{\text{Pr}}
- \newcommand{\E}{\text{E}}
- \newcommand{\Cov}{\text{Cov}}
- \newcommand{\corr}{\text{corr}}
- \newcommand{\Var}{\text{Var}}
- \newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
- \newcommand{\tr}{\text{tr}}
- \newcommand{\bm}{\mathbf}
- \newcommand{\bs}{\boldsymbol}
---



# A fixed effects model

For data that can be grouped into $m$ clusters of observations, @pustejovsky2018small considered the model
\begin{equation}
\label{eq:regression}
\bm{y}_i = \bm{R}_i \bs\beta + \bm{S}_i \bs\gamma + \bm{T}_i \bs\mu + \bs\epsilon_i,
\end{equation}
where $\bm{y}_i$ is an $n_i \times 1$ vector of responses for cluster $i$, $\bm{R}_i$ is an $n_i \times r$ matrix of focal predictors, $\bm{S}_i$ is an $n_i \times s$ matrix of additional covariates that vary across multiple clusters, and $\bm{T}_i$ is an $n_i \times t$ matrix encoding cluster-specific fixed effects, all for $i = 1,...,m$. The cluster-specific fixed effects satisfy $\bm{T}_h \bm{T}_i' = \bm{0}$ for $h \neq i$. Interest centers on inference for the coefficients on the focal predictors $\bs\beta$. 

@pustejovsky2018small considered estimation of Model \ref{eq:regression} by generalized or weighted least squares (WLS). Let $\bm{W}_1,...,\bm{W}_m$ be a set of symmetric weight matrices used for WLS estimation, which may include off-diagonal elements. The CR2 variance estimator involves specifying a working model for the structure of the errors. Consider a working model $\Var\left(\bs\epsilon_i | \bm{R}_i, \bm{S}_i, \bm{T}_i\right) = \sigma^2 \bs\Phi_i$ where $\bs\Phi_i$ is a symmetric $n_i \times n_i$ matrix that may be a function of a low-dimensional, estimable parameter. In some applications, the weight matrices might be taken as $\bm{W}_i = \bs{\hat\Phi}_i^{-1}$, where $\bs{\hat\Phi}_i$ is an estimate of $\bs\Phi_i$. In other applications, the weight matrices may be something else, such as diagonal matrices consisting of sampling weights or identity matrices (i.e., ordinary least squares).

# The CR2 variance estimator 

@pustejovsky2018small provided a generalization of the bias-reduced linearization estimator introduced by @McCaffrey2001generalizations and @Bell2002bias that can be applied to Model \ref{eq:regression}
We follow the same notation as @pustejovsky2018small to define this variance estimator, referred to as CR2. Let $N = \sum_{i=1}^m n_i$ be the total sample size. Let $\bm{U}_i = \left[ \bm{R}_i \ \bm{S}_i \right]$ be the set of predictors that vary across clusters and $\bm{X}_i = \left[ \bm{R}_i \ \bm{S}_i \ \bm{T}_i \right]$ be the full set of predictors. Let $\bm{R}$, $\bm{S}$, $\bm{T}$, $\bm{U}$, $\bm{X}$, and $\bm{y}$ denote the stacked versions of the cluster-specific matrices (i.e., $\bm{R} = \left[\bm{R}_1' \ \bm{R}_2' \ \cdots \ \bm{R}_m'\right]'$, etc.). Let $\bm{W} = \bigoplus_{i=1}^m \bm{W}_i$ and $\bs\Phi = \bigoplus_{i=1}^m \bs\Phi_i$. For a generic matrix $\bm{Z}$, let $\bm{M}_{Z} = \left(\bm{Z}'\bm{W}\bm{Z}\right)^{-1}$ and $\bm{H}_{\bm{Z}} = \bm{Z} \bm{M}_{\bm{Z}}\bm{Z}'\bm{W}$. Let $\bm{C}_i$ be the $n_i \times N$ matrix that selects the rows of cluster $i$ from the full set of observations, such that $\bm{X}_i = \bm{C}_i \bm{X}$. Finally, let $\bm{D}_i$ be the upper-right Cholesky factorization of $\bm{\Phi}_i$. 

These operators provide a means to define absorbed versions of the predictors and the outcome. Let $\bm{\ddot{S}} = \left(\bm{I} - \bm{H}_{\bm{T}}\right) \bm{S}$ be the covariates after absorbing the cluster-specific effects, let $\bm{\ddot{U}} = \left(\bm{I} - \bm{H}_{\bm{T}}\right) \bm{U}$ be an absorbed version of the focal predictors and the covariates, let $\bm{\ddot{R}} = \left(\bm{I} - \bm{H}_{\bm{\ddot{S}}}\right)\left(\bm{I} - \bm{H}_{\bm{T}}\right) \bm{R}$ be the focal predictors after absorbing the covariates and the cluster-specific fixed effects, and let $\bm{e} = \left(\bm{I} - \bm{H}_{\bm{\ddot{R}}}\right)\left(\bm{I} - \bm{H}_{\bm{\ddot{S}}}\right)\left(\bm{I} - \bm{H}_{\bm{T}}\right) \bm{y}$ denote the vector of residuals, with $\bm{e}_i = \bm{C}_i\bm{e}$ denoting the vector of residuals from cluster $i$.  

With this notation established, the CR2 variance estimator has the form 
\begin{equation}
\bm{V}^{CR2} = \bm{M}_{\bm{\ddot{R}}} \left(\sum_{i=1}^m \bm{\ddot{R}}_i' \bm{W}_i \bm{A}_i \bm{e}_i \bm{e}_i' \bm{A}_i \bm{W}_i \bm{\ddot{R}}_i \right) \bm{M}_{\bm{\ddot{R}}},
\end{equation}
where $\bm{\ddot{R}}_i = \bm{C}_i \bm{\ddot{R}}$ is the cluster-specific matrix of absorbed focal predictors and $\bm{A}_1,...,\bm{A}_m$ are a set of adjustment matrices that correct the bias of the residual cross-products.

The adjustment matrices are calculated as follows. Define the matrices 
\begin{equation}
\label{eq:B-matrix}
\bm{B}_i = \bm{D}_i \bm{C}_i \left(\bm{I} - \bm{H}_{\bm{X}}\right) \bs\Phi \left(\bm{I} - \bm{H}_{\bm{X}}\right)'\bm{C}_i' \bm{D}_i'
\end{equation}
for $i = 1,...,m$. The adjustment matrices are then calculated as
\begin{equation}
\label{eq:A-matrix}
\bm{A}_i = \bm{D}_i' \bm{B}_i^{+1/2} \bm{D}_i,
\end{equation}
where $\bm{B}_i^{+1/2}$ is the symmetric square root of the Moore-Penrose inverse of $\bm{B}_i$. 
Theorem 1 of @pustejovsky2018small shows that, if the working model $\bs\Phi$ is correctly specified and some conditions on the rank of $\bm{U}$ are satisfied, then the CR2 estimator is exactly unbiased for the sampling variance of the weighted least squares estimator of $\bs\beta$. Moreover, although the CR2 estimator is defined based on a working model, it remains close to unbiased and outperforms alternative sandwich estimators even when the working model is not correctly specified.

# The original statement of Theorem 2 {#original}

The adjustment matrices given in (\ref{eq:A-matrix}) can be expensive to compute directly because the $\bm{B}_i$ matrices involve computing a "residualized" version of the $N \times N$ matrix $\bs\Phi$ involving the full set of predictors $\bm{X}$---including the cluster-specific fixed effects $\bm{T}_1,...,\bm{T}_m$. Theorem 2 considered whether one can take a computational short cut by omitting the cluster-specific fixed effects from the calculation of the $\bm{B}_i$ matrices. Specifically, define the modified matrices
\begin{equation}
\label{eq:B-modified}
\bm{\tilde{B}}_i = \bm{D}_i \bm{C}_i \left(\bm{I} - \bm{H}_{\bm{\ddot{U}}}\right) \bs\Phi \left(\bm{I} - \bm{H}_{\bm{\ddot{U}}}\right)'\bm{C}_i' \bm{D}_i'
\end{equation}
and 
\begin{equation}
\label{eq:A-modified}
\bm{\tilde{A}}_i = \bm{D}_i' \bm{\tilde{B}}_i^{+1/2} \bm{D}_i.
\end{equation}
Theorem 2 claimed that if the weight matrices are inverse of the working model, such that $\bm{W}_i = \bs\Phi_i^{-1}$ for $i = 1,...,m$, then $\bm{\tilde{B}}_i^{+1/2} = \bm{B}_i^{+1/2}$ and hence $\bm{\tilde{A}}_i = \bm{A}_i$. The implication is that the cluster-specific fixed effects can be ignored when calculating the adjustment matrices. However, the claimed equivalence does not hold in general. The proof of Theorem 2 as given in the supplementary materials of @pustejovsky2018small relied on a Woodbury identity for generalized inverses that does not hold for $\bm{B}_i$ because necessary rank conditions are not satisfied.

We describe a simple numerical example that contradicts the original statement of Theorem 2. Consider a design with $m = 3$ clusters, of sizes $n_1 = 2$, $n_2 = 3$, and $n_3 = 5$ for which we have the model
$$
y_{it} = \beta_0 \times t + \mu_i + \epsilon_{it},
$$
where the errors are heteroskedastic with $\Var(\epsilon_{it}) = \alpha \times t$. We then have $\bm{y}_i = \left[ y_{i1} \cdots y_{in_i}\right]'$, $\bm{R}_i = \left[1 \ 2 \ \cdots \ n_i\right]'$, $\bm{T} = \bigoplus_{i=1}^3 \bm{1}_i$, and $\bs\Phi_i = \text{diag}(1,2,...,n_i)$. 

If the model is estimated using inverse variance weights, so that $\bm{W}_i = \text{diag}\left(1, \frac{1}{2}, \cdots, \frac{1}{n_i}\right)$, then the CR2 adjustment matrices differ depending on whether they are calculated from the full model or from the model after absorbing the fixed effects. Table \ref{tab:example} reports the product of the adjustment matrices and the absorbed design matrix for the weighted least squares estimator. The column labelled Full uses the adjustment matrices based on the full design (i.e., $\bm{A}_i \bm{W}_i \bm{\ddot{R}}_i$, with $\bm{A}_i$ calculated from Equation \ref{eq:A-matrix}) or based on the absorbed design (i.e., $\bm{\tilde{A}}_i \bm{W}_i \bm{\ddot{R}}_i$, with $\bm{\tilde{A}}_i$ calculated from Equation \ref{eq:A-modified}). The values differ, contradicting the original statement of Theorem 2. The final row of the table reports the value of $V^{CR2}$ based on fitting the model using the outcomes reported in the second column of the table. The difference in adjustment matrices leads to differences in the value of the variance estimator.

```{r example-source, echo = FALSE, results = "hide", message = FALSE, warning = FALSE}
source("R/Numerical counter-example to Theorem 2.R", local = knitr::knit_global())
```

```{r example, echo = FALSE, message = FALSE, warning = FALSE}
library(kableExtra)

options(knitr.kable.NA = '')
subset(all_res, select = -ols_absorb) |>
  kable(
    caption = "Adjustment matrices based on weighted or unweighted least squares, calculated with or without absorbing fixed effects",
    col.names = c("Cluster", "Y", "Full", "Absorbed", "Full/Absorbed", "Full", "Absorbed"),
    digits = 3, 
    booktabs = TRUE,
    escape = FALSE
  ) |>
  kable_styling(full_width = FALSE) |>
  collapse_rows(columns = 1:2, latex_hline = "none", valign = "top") |>
  add_header_above(c(" " = 2, "Weighted" = 2, "Unweighted (Hom.)" = 1, "Unweighted (Het.)" = 2)) |>
  row_spec(10, hline_after = TRUE)
```

# A revised Theorem 2 {#revised}

The implication of the original Theorem 2 was that using the modified adjustment matrices $\tilde{\bm{A}}_i$ to calculate the CR2 estimator yields the same result as using the full adjustment matrices $\bm{A}_i$. Although this does not hold under the general conditions given above, a modified version of the theorem does hold for the more limited case of ordinary (unweighted) least squares regression with a homoskedastic working model. The precise conditions are given in the following theorem, with proof given in Section \ref{proof}.

\begin{thm}
\label{thm:absorb}
Let $\bm{L}_i = \left(\bm{\ddot{U}}'\bm{\ddot{U}} - \bm{\ddot{U}}_i'\bm{\ddot{U}}_i\right)$ and assume that $\bm{L}_1,...,\bm{L}_m$ have full rank $r + s$. If $\bm{W}_i = \bm{I}_i$ and $\bs\Phi_i = \bm{I}_i$ for $i = 1,...,m$ and $\bm{T}_i \bm{T}_k' = \bm{0}$ for $i \neq k$, then $\bm{A}_i \bm{\ddot{R}}_i = \bm{\tilde{A}}_i \bm{\ddot{R}}_i$, where $\bm{A}_i$ and $\tilde{\bm{A}}_i$ are as defined in (\ref{eq:A-matrix}) and (\ref{eq:A-modified}), respectively.
\end{thm}

The implication of the revised theorem is that, for ordinary least squares regression with a homoskedastic working model, calculating the CR2 with the modified adjustment matrices $\tilde{\bm{A}}_i$ leads to the same result as using the full adjustment matrices $\bm{A}_i$. 
The equality between the full and absorbed adjustment matrices does not hold for weighted or generalized least squares, nor for ordinary least squares with working models other than $\bs\Phi_i = \bm{I}_i$.

Continuing the example described in the previous section, Table \ref{tab:example} reports the product of the adjustment matrices and the absorbed design matrix for the ordinary least squares estimator with a homoskedastic working model in the column labelled Unweighted (Hom.). The values based on the full design matrix $\left(\bm{A}_i \bm{\ddot{R}}_i\right)$ are numerically identical to the values based on the absorbed design matrix $\left(\bm{\tilde{A}}_i \bm{\ddot{R}}_i\right)$. In the columns labeled Unweighted (Het.), the same quantities are computed for the ordinary least squares estimator, but with the heteroskedastic working model described in the previous section. The values based on the full design matrix differ from the values based on the absorbed design matrix, leading to differences in the cluster-robust variance estimator reported in the final row of the table.

## Remarks

The revised version of Theorem 2 holds for the class of linear models estimated using ordinary least squares, with the cluster-robust variance estimator constructed based on a homoskedastic working model. Considering the ubiquity of ordinary least squares in applied data analysis, this is clearly an important class of estimators---perhaps even the most important for application. Indeed, recent methodological work on small-sample adjustments to cluster-robust variance estimators has focused almost exclusively on ordinary least squares [e.g., @Ibragimov2016inference; @Imbens2015robust; @MacKinnon2016wild]. 

Other important classes of estimators fall outside the scope of Theorem 2. For instance, an analyst might prefer to estimate a linear model using weighted least squares based on a posited heteroskedastic variance structure, paired with heteroskedasticity- or cluster-robust variance estimators to buttress against misspecification of that variance structure [@romano2017resurrecting]. In other applications, an analyst might use generalized estimating equation with a compound symmetric or auto-regressive working model for the errors [@Liang1986longitudinal; @wang2003working]. In still other applications, analysts might need to estimate a linear model using survey weights or inverse propensity weights, while maintaining a homoskedastic working model. For such applications, efficient computation of CR2 or other small-sample adjusted cluster-robust variance estimators remains a topic for further research.

## Proof {#proof}

Setting $\bs\Phi_i = \bm{I}_i$ and observing that $\bm{\ddot{U}}_i'\bm{T}_i = \bm{0}$ for $i = 1,...,m$, it follows that
\begin{align}
\bm{B}_i &= \bm{D}_i \bm{C}_i \left(\bm{I} - \bm{H_{\ddot{U}}}\right) \left(\bm{I} - \bm{H_T}\right) \bs\Phi \left(\bm{I} - \bm{H_T}\right)' \left(\bm{I} - \bm{H_{\ddot{U}}}\right)' \bm{C}_i' \bm{D}_i' \nonumber \\ 
&= \bm{C}_i \left(\bm{I} - \bm{H_{\ddot{U}}} - \bm{H_T}\right) \left(\bm{I} - \bm{H_{\ddot{U}}} - \bm{H_T}\right) \bm{C}_i' \nonumber\\ 
\label{eq:B_i}
&= \left(\bm{I}_i - \bm{\ddot{U}}_i \bm{M_{\ddot{U}}}\bm{\ddot{U}}_i' - \bm{T}_i \bm{M_T}\bm{T}_i'\right)
\end{align}
and similarly,
\begin{equation}
\label{eq:Btilde_i}
\tilde{\bm{B}}_i = \left(\bm{I}_i - \bm{\ddot{U}}_i \bm{M_{\ddot{U}}}\bm{\ddot{U}}_i'\right).
\end{equation}

It is apparent that $\tilde{\bm{B}}_i \bm{T}_i = \bm{T}_i$. We now show that $\tilde{\bm{A}}_i \bm{T}_i = \bm{T}_i$ as well. Denote the rank of $\bm{\ddot{U}}_i$ as $u_i \leq \min \left\{n_i, r + s \right\}$ and take the thin QR decomposition of $\bm{\ddot{U}}_i$ as $\bm{\ddot{U}}_i = \bm{Q}_i \bm{R}_i$, where $\bm{Q}_i$ is an $n_i \times u_i$ semi-orthonormal matrix and $\bm{R}_i$ is a $u_i \times r + s$ matrix of rank $u_i$, with $\bm{Q}_i'\bm{Q}_i = \bm{I}$. Note that $\bm{Q}_i'\bm{T}_i = \bm{0}$. From the observation that $\tilde{\bm{B}}_i$ can be written as 
$$
\tilde{\bm{B}}_i = \bm{I}_i - \bm{Q}_i \bm{Q}_i' + \bm{Q}_i \left(\bm{I} - \bm{R}_i \bm{M}_{\bm{\ddot{U}}} \bm{R}_i'\right)\bm{Q}_i',
$$
it can be seen that
\begin{equation}
\tilde{\bm{A}}_i = \tilde{\bm{B}}_i^{+1/2} = \bm{I}_i - \bm{Q}_i \bm{Q}_i' + \bm{Q}_i \left(\bm{I} - \bm{R}_i \bm{M}_{\bm{\ddot{U}}} \bm{R}_i'\right)^{+1/2} \bm{Q}_i'.
\end{equation}
It follows that $\tilde{\bm{A}}_i \bm{T}_i = \bm{T}_i$.

Define the matrices
\begin{equation}
\bm{A}_i = \tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i',
\end{equation}
for $i = 1,...,m$. We claim that $\bm{A}_i = \bm{B}_i^{+1/2}$. This can be seen by observing that $\bm{A}_i \bm{A}_i$ is equal to $\bm{B}_i^+$, the Moore-Penrose inverse of $\bm{B}_i$. Note that
\begin{equation}
\bm{A}_i \bm{A}_i = \tilde{\bm{A}}_i \tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i' =  = \tilde{\bm{B}}_i^+ - \bm{T}_i \bm{M_T}\bm{T}_i'.
\end{equation}
It can then readily be verified that
i) $\bm{B}_i \bm{A}_i \bm{A}_i \bm{B}_i = \bm{B}_i$,
ii) $\bm{A}_i \bm{A}_i \bm{B}_i \bm{A}_i \bm{A}_i = \bm{A}_i \bm{A}_i$, and
iii) $\bm{A}_i \bm{A}_i \bm{B}_i = \bm{B}_i \bm{A}_i \bm{A}_i$.
Thus, $\bm{A}_i\bm{A}_i$ satisfies the definition of the Moore-Penrose inverse of $\bm{B}_i$ [@rao1971generalized].

Finally, because $\bm{T}_i ' \bm{\ddot{R}}_i= \bm{0}$, it can be seen that $\bm{A}_i \bm{\ddot{R}}_i = \left(\tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i'\right)\bm{\ddot{R}}_i = \tilde{\bm{A}}_i \bm{\ddot{R}}_i$.

# Acknowledgements {-}

We are grateful to Michael Pfaffenmayr for drawing our attention to the error in the earlier version of Theorem 2.

The authors report there are no competing interests to declare. 

# Supplementary materials {-}

The supplementary materials include R code for reproducing the numerical example described in Sections \ref{original} and \ref{revised}.

# References {-}
