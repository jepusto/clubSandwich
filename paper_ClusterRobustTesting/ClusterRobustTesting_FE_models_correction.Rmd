---
title: "Corrigendum: Small sample methods for cluster-robust variance estimation and hypothesis testing in fixed effects models"
blinded: 0
authors:
  - name: James E. Pustejovsky
    affiliation: University of Wisconsin - Madison
    thanks: "Department of Educational Psychology, University of Wisconsin - Madison, 1025 West Johnson Street, Madison, WI 53706. Email: pustejovsky@wisc.edu"
  - name: Elizabeth Tipton
    affiliation: Northwestern University
    thanks: "Department of Statistics, Northwestern University. Email: tipton@northwestern.edu"
bibliography: bibliography.bib
biblio-style: agsm
keep_tex: yes
output:
  rticles::asa_article: 
    citation_package: default
    number_sections: false
abstract: |
  @pustejovsky2018small considered how to implement cluster-robust variance estimators for fixed effects models estimated by weighted (or unweighted) least squares. Theorem 2 of the paper concerns a computational short cut for a certain cluster-robust variance estimator in models with cluster-specific fixed effects. It claimed that this short cut works for models estimated by generalized least squares, as long as the weights are taken to be inverse of the working model. However, the theorem is incorrect. In this corrigendum, we  review the CR2 variance estimator, describe the assertion of the theorem as originally stated, and explain the error. We then provide a revised version of the theorem, which holds only under more limited conditions, for models estimated by ordinary least squares.

header-includes:
- \usepackage{amsthm}
- \newtheorem*{thm}{Theorem}
- \newtheorem{lem}{Lemma}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage[natbibapa]{apacite}
- \usepackage{caption}
- \usepackage{multirow}
- \usepackage{float}    % for fig.pos='H'
- \usepackage{rotfloat} % for sidewaysfigure
- \newcommand{\Prob}{\text{Pr}}
- \newcommand{\E}{\text{E}}
- \newcommand{\Cov}{\text{Cov}}
- \newcommand{\corr}{\text{corr}}
- \newcommand{\Var}{\text{Var}}
- \newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
- \newcommand{\tr}{\text{tr}}
- \newcommand{\bm}{\mathbf}
- \newcommand{\bs}{\boldsymbol}
---



# A fixed effects model

For data that can be grouped into $m$ clusters of observations, @pustejovsky2018small considered the model
\begin{equation}
\label{eq:regression}
\bm{y}_i = \bm{R}_i \bs\beta + \bm{S}_i \bs\gamma + \bm{T}_i \bs\mu + \bs\epsilon_i,
\end{equation}
where $\bm{y}_i$ is an $n_i \times 1$ vector of responses for cluster $i$, $\bm{R}_i$ is an $n_i \times r$ matrix of focal predictors, $\bm{S}_i$ is an $n_i \times s$ matrix of additional covariates that vary across multiple clusters, and $\bm{T}_i$ is an $n_i \times t$ matrix encoding cluster-specific fixed effects, all for $i = 1,...,m$. The cluster-specific fixed effects satisfy $\bm{T}_h \bm{T}_i' = \bm{0}$ for $h \neq i$. Interest centers on inference for the coefficients on the focal predictors $\bs\beta$. 

@pustejovsky2018small considered estimation of Model \ref{eq:regression} by weighted least squares (WLS). Let $\bm{W}_1,...,\bm{W}_m$ be a set of symmetric weight matrices used for WLS estimation. The CR2 variance estimator involves specifying a working model for the structure of the errors. Consider a working model $\Var\left(\bs\epsilon_i | \bm{R}_i, \bm{S}_i, \bm{T}_i\right) = \sigma^2 \bs\Phi_i$ where $\bs\Phi_i$ is a symmetric $n_i \times n_i$ matrix that may be a function of a low-dimensional, estimable parameter. In some applications, the weight matrices might be taken as $\bm{W}_i = \bs{\hat\Phi}_i^{-1}$, where $\bs{\hat\Phi}_i$ is an estimate of $\bs\Phi_i$. In other applications, the weight matrices may be something else, such as diagonal matrices consisting of sampling weights or identity matrices (i.e., ordinary least squares).

# The CR2 variance estimator 

@pustejovsky2018small provided a generalization of the bias-reduced linearization estimator introduced by @McCaffrey2001generalizations and @Bell2002bias that can be applied to Model \ref{eq:regression}, referred to as the CR2 variance estimator. 
We follow the same notation as @pustejovsky2018small to define CR2. Let $N = \sum_{i=1}^m n_i$ be the total sample size. Let $\bm{U}_i = \left[ \bm{R}_i \ \bm{S}_i \right]$ be the set of predictors that vary across clusters and $\bm{X}_i = \left[ \bm{R}_i \ \bm{S}_i \ \bm{T}_i \right]$ be the full set of predictors. Let $\bm{R}$, $\bm{S}$, $\bm{T}$, $\bm{U}$, $\bm{X}$, and $\bm{y}$ denote the stacked versions of the cluster-specific matrices (i.e., $\bm{R} = \left[\bm{R}_1' \ \bm{R}_2' \ \cdots \ \bm{R}_m'\right]'$, etc.). Let $\bm{W} = \bigoplus_{i=1}^m \bm{W}_i$ and $\bs\Phi = \bigoplus_{i=1}^m \bs\Phi_i$. For a generic matrix $\bm{Z}$, let $\bm{M}_{Z} = \left(\bm{Z}'\bm{W}\bm{Z}\right)^{-1}$ and $\bm{H}_{\bm{Z}} = \bm{Z} \bm{M}_{\bm{Z}}\bm{Z}'\bm{W}$. Let $\bm{C}_i$ be the $n_i \times N$ matrix that selects the rows of cluster $i$ from the full set of observations, such that $\bm{X}_i = \bm{C}_i \bm{X}$. Finally, let $\bm{D}_i$ be the upper-right Cholesky factorization of $\bm{\Phi}_i$. 

These operators provide a means to define absorbed versions of the predictors and the outcome. Let $\bm{\ddot{S}} = \left(\bm{I} - \bm{H}_{\bm{T}}\right) \bm{S}$ be the covariates after absorbing the cluster-specific effects, let $\bm{\ddot{U}} = \left(\bm{I} - \bm{H}_{\bm{T}}\right) \bm{U}$ be an absorbed version of the focal predictors and the covariates, let $\bm{\ddot{R}} = \left(\bm{I} - \bm{H}_{\bm{\ddot{S}}}\right)\left(\bm{I} - \bm{H}_{\bm{T}}\right) \bm{R}$ be the focal predictors after absorbing the covariates and the cluster-specific fixed effects, and let $\bm{e} = \left(\bm{I} - \bm{H}_{\bm{\ddot{R}}}\right)\left(\bm{I} - \bm{H}_{\bm{\ddot{S}}}\right)\left(\bm{I} - \bm{H}_{\bm{T}}\right) \bm{y}$ denote the vector of residuals, with $\bm{e}_i = \bm{C}_i\bm{e}$.  

With this notation established, the CR2 variance estimator has the form 
\begin{equation}
\bm{V}^{CR2} = \bm{M}_{\bm{\ddot{R}}} \left(\sum_{i=1}^m \bm{\ddot{R}}_i' \bm{W}_i \bm{A}_i \bm{e}_i \bm{e}_i' \bm{A}_i \bm{W}_i \bm{\ddot{R}}_i \right) \bm{M}_{\bm{\ddot{R}}},
\end{equation}
where $\bm{\ddot{R}}_i = \bm{C}_i \bm{\ddot{R}}$ is the cluster-specific matrix of absorbed focal predictors, $\bm{e}_i$ is the vector of weighted least squares residuals from cluster $i$, and $\bm{A}_1,...,\bm{A}_m$ are a set of adjustment matrices that correct the bias of the residual cross-products.

The adjustment matrices are calculated as follows. Define the matrices 
\begin{equation}
\label{eq:B-matrix}
\bm{B}_i = \bm{D}_i \bm{C}_i \left(\bm{I} - \bm{H}_{\bm{X}}\right) \bs\Phi \left(\bm{I} - \bm{H}_{\bm{X}}\right)'\bm{C}_i' \bm{D}_i'
\end{equation}
for $i = 1,...,m$. The adjustment matrices are then calculated as
\begin{equation}
\label{eq:A-matrix}
\bm{A}_i = \bm{D}_i' \bm{B}_i^{+1/2} \bm{D}_i,
\end{equation}
where $\bm{B}_i^{+1/2}$ is the symmetric square root of the Moore-Penrose inverse of $\bm{B}_i$. 
Theorem 1 of @pustejovsky2018small shows that, if the working model $\bs\Phi$ is correctly specified and some conditions on the rank of $\bm{U}$ are satisfied, then the CR2 estimator is exactly unbiased for the sampling variance of the weighted least squares estimator of $\bs\beta$. Moreover, although the CR2 estimator is defined based on a working model, it remains close to unbiased and outperforms alternative sandwich estimators even when the working model is not correctly specified.

# The original statement of Theorem 2

The adjustment matrices given in (\ref{eq:A-matrix}) can be expensive to compute directly because the $\bm{B}_i$ matrices involve computing a "residualized" version of the $N \times N$ matrix $\bs\Phi$ involving the full set of predictors $\bm{X}$---including the cluster-specific fixed effects $\bm{T}_1,...,\bm{T}_m$. Theorem 2 considered whether one can take a computational short cut by omitting the cluster-specific fixed effects from the calculation of the $\bm{B}_i$ matrices. Specifically, define the modified matrices
\begin{equation}
\label{eq:B-modified}
\bm{\tilde{B}}_i = \bm{D}_i \bm{C}_i \left(\bm{I} - \bm{H}_{\bm{\ddot{U}}}\right) \bs\Phi \left(\bm{I} - \bm{H}_{\bm{\ddot{U}}}\right)'\bm{C}_i' \bm{D}_i'
\end{equation}
and 
\begin{equation}
\label{eq:A-modified}
\bm{\tilde{A}}_i = \bm{D}_i' \bm{\tilde{B}}_i^{+1/2} \bm{D}_i.
\end{equation}
Theorem 2 claimed that if the weight matrices are inverse of the working model, such that $\bm{W}_i = \bs\Phi_i^{-1}$ for $i = 1,...,m$, then $\bm{\tilde{B}}_i^{+1/2} = \bm{B}_i^{+1/2}$ and hence $\bm{\tilde{A}}_i = \bm{A}_i$. The implication is that the cluster-specific fixed effects can be ignored when calculating the adjustment matrices. However, the claimed equivalence does not actually hold. The proof of Theorem 2 as given in the supplementary materials of @pustejovsky2018small relied on a Woodbury identity for generalized inverses that does not hold for $\bm{B}_i$ because necessary rank conditions are not satisfied.

We describe a simple numerical example that contradicts the original statement of Theorem 2. Consider a block-randomized experiment with $m = 3$ blocks, of sizes $n_1 = 2$, $n_2 = 4$, and $n_3 = 6$. In each block, a single observation receives treatment and the remaining observations receive the control condition, so that $\bm{R}_i = \left[1 \ 0 \cdots 0\right]'$, $\bm{T}$ consists of indicators for each block, and there are no additional covariates. The model is then
$$
\bm{y}_i = \bm{R}_i \beta + \mu_i + \bs\epsilon_i
$$
Assume that treatment impacts are heterogeneous such that $\Var(\bs\epsilon_{1i}) = \sigma^2 + \tau^2$ and $\Var(\bs\epsilon_{ji}) = \sigma^2$ for $j > 1$, where $\tau^2$ is known. Consider estimating the model using weighted least squares with inverse-variance weights. 

```{r}
set.seed(20230114)
library(clubSandwich)
ni <- c(2,3,5)
m <- length(ni)
ri <- lapply(ni, \(x) 1:x)
wi <- lapply(ni, \(x) 1 / (1:x))
rddi_ols <- lapply(ri, \(r) r - mean(r))
rddi_wt <- mapply(\(r, w) r - weighted.mean(r,w = w), r = ri, w = wi)

clust <- factor(rep(LETTERS[1:m], ni))
mu <- 0.2 * unlist(ri) + ni[clust]
yi <- round(rnorm(sum(ni), mean = mu, sd = sqrt(unlist(ri))), 1)
yddi_ols <- tapply(yi, clust, \(x) x - mean(x))
yddi_wt <- mapply(\(y,w) y - weighted.mean(y, w = w), y = split(yi, clust), w = wi)

dat <- data.frame(
  y = yi, 
  y_dd_ols = unlist(yddi_ols),
  y_dd_wt = unlist(yddi_wt),
  R = unlist(ri),
  R_dd_ols = unlist(rddi_ols),
  R_dd_wt = unlist(rddi_wt),
  w = unlist(wi),
  clust = clust
)

ols_fit <- lm(y ~ clust + R, data = dat)
V_ols_full <- vcovCR(ols_fit, type = "CR2", cluster = dat$clust)
A_ols_full <- attr(V_ols_full,"adjustments")
emat_ols_full <- mapply(\(r, a) a %*% r, r = rddi_ols, a = A_ols_full)

ols_absorb <- lm(y_dd_ols ~ 0 + R_dd_ols, data = dat)
V_ols_absorb <- vcovCR(ols_absorb, type = "CR2", cluster = dat$clust)
A_ols_absorb <- attr(V_ols_absorb, "adjustments")
emat_ols_absorb <- mapply(\(r, a) a %*% r, r = rddi_ols, a = A_ols_absorb)

wls_fit <- lm(y ~ clust + R, weights = w, data = dat)
V_wt_full <- vcovCR(wls_fit, type = "CR2", cluster = dat$clust, inverse_var = TRUE)
A_wt_full <- attr(V_wt_full, "adjustments")
emat_wt_full <- mapply(\(r, w, a) a %*% (w * r), r = rddi_wt, w = wi, a = A_wt_full)

wls_absorbed <- lm(y_dd_wt ~ 0 + R_dd_wt, weights = w, data = dat)
V_wt_absorb <- vcovCR(wls_absorbed, type = "CR2", cluster = dat$clust, inverse_var = TRUE)
A_wt_absorb <- attr(V_wt_absorb, "adjustments")
emat_wt_absorb <- mapply(\(r, w, a) a %*% (w * r), r = rddi_wt, w = wi, a = A_wt_absorb)

RWA <- 
  data.frame(
    cluster = clust,
    y = yi,
    wt_full = unlist(emat_wt_full),
    wt_absorb = unlist(emat_wt_absorb),
    ols_full = unlist(emat_ols_full),
    ols_absorb = unlist(emat_ols_absorb)
  )

V_comp <- 
  data.frame(
    cluster = "V^CR",
    y = NA,
    wt_full = V_wt_full["R","R"],
    wt_absorb = V_wt_absorb[1,1],
    ols_full = V_ols_full["R","R"],
    ols_absorb = V_ols_absorb[1,1]
  )

subset(rbind(RWA, V_comp), select = -ols_absorb)
```

# A revised Theorem 2

The implication of the original Theorem 2 was that using the modified adjustment matrices $\tilde{\bm{A}}_i$ to calculate the CR2 estimator yields the same result as using the full adjustment matrices $\bm{A}_i$. Although this does not hold under the general conditions given above, a modified version of the theorem does hold for the more limited case of ordinary (unweighted) least squares regression with an "independence" working model. The precise conditions are given in the following theorem.  

\begin{thm}
\label{thm:absorb}
Let $\bm{L}_i = \left(\bm{\ddot{U}}'\bm{\ddot{U}} - \bm{\ddot{U}}_i'\bm{\ddot{U}}_i\right)$ and assume that $\bm{L}_1,...,\bm{L}_m$ have full rank $r + s$. If $\bm{W}_i = \bm{I}_i$ and $\bs\Phi_i = \bm{I}_i$ for $i = 1,...,m$ and $\bm{T}_i \bm{T}_k' = \bm{0}$ for $i \neq k$, then $\bm{A}_i \bm{\ddot{R}}_i = \bm{\tilde{A}}_i \bm{\ddot{R}}_i$, where $\bm{A}_i$ and $\tilde{\bm{A}}_i$ are as defined in (\ref{eq:A-matrix}) and (\ref{eq:A-modified}), respectively.
\end{thm}

The implication of the revised theorem is that, for ordinary least squares regression with an "independence" working model, calculating the CR2 with the modified adjustment matrices $\tilde{\bm{A}}_i$ leads to the same result as using the full adjustment matrices $\bm{A}_i$. The equality does not hold for weighted or generalized least squares, nor for ordinary least squares with working models other than $\bs\Phi_i = \bm{I}_i$.

## Proof

Setting $\bs\Phi_i = \bm{I}_i$ and observing that $\bm{\ddot{U}}_i'\bm{T}_i = \bm{0}$ for $i = 1,...,m$, it follows that
\begin{align}
\bm{B}_i &= \bm{D}_i \bm{C}_i \left(\bm{I} - \bm{H_{\ddot{U}}}\right) \left(\bm{I} - \bm{H_T}\right) \bs\Phi \left(\bm{I} - \bm{H_T}\right)' \left(\bm{I} - \bm{H_{\ddot{U}}}\right)' \bm{C}_i' \bm{D}_i' \nonumber \\ 
&= \bm{C}_i \left(\bm{I} - \bm{H_{\ddot{U}}} - \bm{H_T}\right) \left(\bm{I} - \bm{H_{\ddot{U}}} - \bm{H_T}\right) \bm{C}_i' \nonumber\\ 
\label{eq:B_i}
&= \left(\bm{I}_i - \bm{\ddot{U}}_i \bm{M_{\ddot{U}}}\bm{\ddot{U}}_i' - \bm{T}_i \bm{M_T}\bm{T}_i'\right)
\end{align}
and similarly,
\begin{equation}
\label{eq:Btilde_i}
\tilde{\bm{B}}_i = \left(\bm{I}_i - \bm{\ddot{U}}_i \bm{M_{\ddot{U}}}\bm{\ddot{U}}_i'\right).
\end{equation}

We now show that $\tilde{\bm{A}}_i \bm{T}_i = \bm{T}_i$. Denote the rank of $\bm{\ddot{U}}_i$ as $u_i \leq \min \left\{n_i, r + s \right\}$ and take the thin QR decomposition of $\bm{\ddot{U}}_i$ as $\bm{\ddot{U}}_i = \bm{Q}_i \bm{R}_i$, where $\bm{Q}_i$ is an $n_i \times u_i$ semi-orthonormal matrix and $\bm{R}_i$ is a $u_i \times r + s$ matrix of rank $u_i$, with $\bm{Q}_i'\bm{Q}_i = \bm{I}$. Note that $\bm{Q}_i'\bm{T}_i = \bm{0}$. From the observation that $\tilde{\bm{B}}_i$ can be written as 
$$
\tilde{\bm{B}}_i = \bm{I}_i - \bm{Q}_i \bm{Q}_i' + \bm{Q}_i \left(\bm{I} - \bm{R}_i \bm{M}_{\bm{\ddot{U}}} \bm{R}_i'\right)\bm{Q}_i',
$$
it can be seen that
\begin{equation}
\tilde{\bm{A}}_i = \tilde{\bm{B}}_i^{+1/2} = \bm{I}_i - \bm{Q}_i \bm{Q}_i' + \bm{Q}_i \left(\bm{I} - \bm{R}_i \bm{M}_{\bm{\ddot{U}}} \bm{R}_i'\right)^{+1/2} \bm{Q}_i'.
\end{equation}
It follows that $\tilde{\bm{A}}_i \bm{T}_i = \bm{T}_i$.

Setting
\begin{equation}
\bm{A}_i = \tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i',
\end{equation}
observe that
\begin{align*}
\bm{B}_i \bm{A}_i \bm{B}_i \bm{A}_i &= \left(\tilde{\bm{B}}_i - \bm{T}_i \bm{M_T}\bm{T}_i'\right) \left(\tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i'\right)\left(\tilde{\bm{B}}_i - \bm{T}_i \bm{M_T}\bm{T}_i'\right) \left(\tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i'\right) \\
&= \left(\tilde{\bm{B}}_i\tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i'\right)\left(\tilde{\bm{B}}_i\tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i'\right) \\
&= \left(\tilde{\bm{B}}_i\tilde{\bm{A}}_i\tilde{\bm{B}}_i\tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i'\right) \\
&= \left(\tilde{\bm{B}}_i - \bm{T}_i \bm{M_T}\bm{T}_i'\right) \\
&= \bm{B}_i.
\end{align*}
It follows that $\bm{A}_i$ is the symmetric square root of the Moore-Penrose inverse of $\bm{B}_i$, i.e., $\bm{A}_i = \bm{B}_i^{+1/2}$. Finally, because $\bm{T}_i ' \bm{\ddot{R}}_i= \bm{0}$, it can be seen that $\bm{A}_i \bm{\ddot{R}}_i = \left(\tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i'\right)\bm{\ddot{R}}_i = \tilde{\bm{A}}_i \bm{\ddot{R}}_i$.

# References {-}
