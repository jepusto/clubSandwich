% !TeX program = pdfLaTeX
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}


\usepackage{textcomp}
\usepackage[hyphens]{url} % not crucial - just used below for the URL
\usepackage{hyperref}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

%% load any required packages here



% tightlist command for lists without linebreak
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


% Pandoc citation processing
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
% for Pandoc 2.8 to 2.10.1
\newenvironment{cslreferences}%
  {}%
  {\par}
% For Pandoc 2.11+
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{amsthm}
\newtheorem*{thm}{Theorem}
\newtheorem{lem}{Lemma}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[natbibapa]{apacite}
\usepackage{caption}
\usepackage{multirow}
\usepackage{float}    % for fig.pos='H'
\usepackage{rotfloat} % for sidewaysfigure
\hypersetup{hidelinks}
\newcommand{\Prob}{\text{Pr}}
\newcommand{\E}{\text{E}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\corr}{\text{corr}}
\newcommand{\Var}{\text{Var}}
\newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\tr}{\text{tr}}
\newcommand{\bm}{\mathbf}
\newcommand{\bs}{\boldsymbol}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\begin{document}


\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Corrigendum: Small sample methods for cluster-robust
variance estimation and hypothesis testing in fixed effects models}

  \author{
        James E. Pustejovsky \thanks{Department of Educational
Psychology, University of Wisconsin - Madison, 1025 West Johnson Street,
Madison, WI 53706. Email:
\href{mailto:pustejovsky@wisc.edu}{\nolinkurl{pustejovsky@wisc.edu}}} \\
    University of Wisconsin - Madison\\
     and \\     Elizabeth Tipton \thanks{Department of Statistics,
Northwestern University. Email:
\href{mailto:tipton@northwestern.edu}{\nolinkurl{tipton@northwestern.edu}}} \\
    Northwestern University\\
      }
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Corrigendum: Small sample methods for cluster-robust
variance estimation and hypothesis testing in fixed effects models}
  \end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
Pustejovsky and Tipton (2018) considered how to implement cluster-robust
variance estimators for fixed effects models estimated by weighted (or
unweighted) least squares. Theorem 2 of the paper concerns a
computational short cut for a certain cluster-robust variance estimator
in models with cluster-specific fixed effects. It claimed that this
short cut works for models estimated by generalized least squares, as
long as the weights are taken to be inverse of the working model.
However, the theorem is incorrect. In this corrigendum, we review the
CR2 variance estimator, describe the assertion of the theorem as
originally stated, and demonstrate the error with a counter-example. We
then provide a revised version of the theorem, which holds for the more
limited set of models estimated by ordinary least squares.
\end{abstract}

\noindent%
{\it Keywords:} 
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\hypertarget{a-fixed-effects-model}{%
\section{A fixed effects model}\label{a-fixed-effects-model}}

For data that can be grouped into \(m\) clusters of observations,
Pustejovsky and Tipton (2018) considered the model \begin{equation}
\label{eq:regression}
\mathbf{y}_i = \mathbf{R}_i \boldsymbol\beta + \mathbf{S}_i \boldsymbol\gamma + \mathbf{T}_i \boldsymbol\mu + \boldsymbol\epsilon_i,
\end{equation} where \(\mathbf{y}_i\) is an \(n_i \times 1\) vector of
responses for cluster \(i\), \(\mathbf{R}_i\) is an \(n_i \times r\)
matrix of focal predictors, \(\mathbf{S}_i\) is an \(n_i \times s\)
matrix of additional covariates that vary across multiple clusters, and
\(\mathbf{T}_i\) is an \(n_i \times t\) matrix encoding cluster-specific
fixed effects, all for \(i = 1,...,m\). The cluster-specific fixed
effects satisfy \(\mathbf{T}_h \mathbf{T}_i' = \mathbf{0}\) for
\(h \neq i\). Interest centers on inference for the coefficients on the
focal predictors \(\boldsymbol\beta\).

Pustejovsky and Tipton (2018) considered estimation of Model
\ref{eq:regression} by generalized or weighted least squares (WLS). Let
\(\mathbf{W}_1,...,\mathbf{W}_m\) be a set of symmetric weight matrices
used for WLS estimation, which may include off-diagonal elements. The
CR2 variance estimator involves specifying a working model for the
structure of the errors. Consider a working model
\(\text{Var}\left(\boldsymbol\epsilon_i | \mathbf{R}_i, \mathbf{S}_i, \mathbf{T}_i\right) = \sigma^2 \boldsymbol\Phi_i\)
where \(\boldsymbol\Phi_i\) is a symmetric \(n_i \times n_i\) matrix
that may be a function of a low-dimensional, estimable parameter. In
some applications, the weight matrices might be taken as
\(\mathbf{W}_i = \boldsymbol{\hat\Phi}_i^{-1}\), where
\(\boldsymbol{\hat\Phi}_i\) is an estimate of \(\boldsymbol\Phi_i\). In
other applications, the weight matrices may be something else, such as
diagonal matrices consisting of sampling weights or identity matrices
(i.e., ordinary least squares).

\hypertarget{the-cr2-variance-estimator}{%
\section{The CR2 variance estimator}\label{the-cr2-variance-estimator}}

Pustejovsky and Tipton (2018) provided a generalization of the
bias-reduced linearization estimator introduced by McCaffrey, Bell, and
Botts (2001) and Bell and McCaffrey (2002) that can be applied to Model
\ref{eq:regression} We follow the same notation as Pustejovsky and
Tipton (2018) to define this variance estimator, referred to as CR2. Let
\(N = \sum_{i=1}^m n_i\) be the total sample size. Let
\(\mathbf{U}_i = \left[ \mathbf{R}_i \ \mathbf{S}_i \right]\) be the set
of predictors that vary across clusters and
\(\mathbf{X}_i = \left[ \mathbf{R}_i \ \mathbf{S}_i \ \mathbf{T}_i \right]\)
be the full set of predictors. Let \(\mathbf{R}\), \(\mathbf{S}\),
\(\mathbf{T}\), \(\mathbf{U}\), \(\mathbf{X}\), and \(\mathbf{y}\)
denote the stacked versions of the cluster-specific matrices (i.e.,
\(\mathbf{R} = \left[\mathbf{R}_1' \ \mathbf{R}_2' \ \cdots \ \mathbf{R}_m'\right]'\),
etc.). Let \(\mathbf{W} = \bigoplus_{i=1}^m \mathbf{W}_i\) and
\(\boldsymbol\Phi = \bigoplus_{i=1}^m \boldsymbol\Phi_i\). For a generic
matrix \(\mathbf{Z}\), let
\(\mathbf{M}_{Z} = \left(\mathbf{Z}'\mathbf{W}\mathbf{Z}\right)^{-1}\)
and
\(\mathbf{H}_{\mathbf{Z}} = \mathbf{Z} \mathbf{M}_{\mathbf{Z}}\mathbf{Z}'\mathbf{W}\).
Let \(\mathbf{C}_i\) be the \(n_i \times N\) matrix that selects the
rows of cluster \(i\) from the full set of observations, such that
\(\mathbf{X}_i = \mathbf{C}_i \mathbf{X}\). Finally, let
\(\mathbf{D}_i\) be the upper-right Cholesky factorization of
\(\mathbf{\Phi}_i\).

These operators provide a means to define absorbed versions of the
predictors and the outcome. Let
\(\mathbf{\ddot{S}} = \left(\mathbf{I} - \mathbf{H}_{\mathbf{T}}\right) \mathbf{S}\)
be the covariates after absorbing the cluster-specific effects, let
\(\mathbf{\ddot{U}} = \left(\mathbf{I} - \mathbf{H}_{\mathbf{T}}\right) \mathbf{U}\)
be an absorbed version of the focal predictors and the covariates, let
\(\mathbf{\ddot{R}} = \left(\mathbf{I} - \mathbf{H}_{\mathbf{\ddot{S}}}\right)\left(\mathbf{I} - \mathbf{H}_{\mathbf{T}}\right) \mathbf{R}\)
be the focal predictors after absorbing the covariates and the
cluster-specific fixed effects, and let
\(\mathbf{e} = \left(\mathbf{I} - \mathbf{H}_{\mathbf{\ddot{R}}}\right)\left(\mathbf{I} - \mathbf{H}_{\mathbf{\ddot{S}}}\right)\left(\mathbf{I} - \mathbf{H}_{\mathbf{T}}\right) \mathbf{y}\)
denote the vector of residuals, with
\(\mathbf{e}_i = \mathbf{C}_i\mathbf{e}\) denoting the vector of
residuals from cluster \(i\).

With this notation established, the CR2 variance estimator has the form
\begin{equation}
\mathbf{V}^{CR2} = \mathbf{M}_{\mathbf{\ddot{R}}} \left(\sum_{i=1}^m \mathbf{\ddot{R}}_i' \mathbf{W}_i \mathbf{A}_i \mathbf{e}_i \mathbf{e}_i' \mathbf{A}_i \mathbf{W}_i \mathbf{\ddot{R}}_i \right) \mathbf{M}_{\mathbf{\ddot{R}}},
\end{equation} where
\(\mathbf{\ddot{R}}_i = \mathbf{C}_i \mathbf{\ddot{R}}\) is the
cluster-specific matrix of absorbed focal predictors and
\(\mathbf{A}_1,...,\mathbf{A}_m\) are a set of adjustment matrices that
correct the bias of the residual cross-products.

The adjustment matrices are calculated as follows. Define the matrices
\begin{equation}
\label{eq:B-matrix}
\mathbf{B}_i = \mathbf{D}_i \mathbf{C}_i \left(\mathbf{I} - \mathbf{H}_{\mathbf{X}}\right) \boldsymbol\Phi \left(\mathbf{I} - \mathbf{H}_{\mathbf{X}}\right)'\mathbf{C}_i' \mathbf{D}_i'
\end{equation} for \(i = 1,...,m\). The adjustment matrices are then
calculated as \begin{equation}
\label{eq:A-matrix}
\mathbf{A}_i = \mathbf{D}_i' \mathbf{B}_i^{+1/2} \mathbf{D}_i,
\end{equation} where \(\mathbf{B}_i^{+1/2}\) is the symmetric square
root of the Moore-Penrose inverse of \(\mathbf{B}_i\). Theorem 1 of
Pustejovsky and Tipton (2018) shows that, if the working model
\(\boldsymbol\Phi\) is correctly specified and some conditions on the
rank of \(\mathbf{U}\) are satisfied, then the CR2 estimator is exactly
unbiased for the sampling variance of the weighted least squares
estimator of \(\boldsymbol\beta\). Moreover, although the CR2 estimator
is defined based on a working model, it remains close to unbiased and
outperforms alternative sandwich estimators even when the working model
is not correctly specified.

\hypertarget{the-original-statement-of-theorem-2}{%
\section{The original statement of Theorem
2}\label{the-original-statement-of-theorem-2}}

The adjustment matrices given in (\ref{eq:A-matrix}) can be expensive to
compute directly because the \(\mathbf{B}_i\) matrices involve computing
a ``residualized'' version of the \(N \times N\) matrix
\(\boldsymbol\Phi\) involving the full set of predictors
\(\mathbf{X}\)---including the cluster-specific fixed effects
\(\mathbf{T}_1,...,\mathbf{T}_m\). Theorem 2 considered whether one can
take a computational short cut by omitting the cluster-specific fixed
effects from the calculation of the \(\mathbf{B}_i\) matrices.
Specifically, define the modified matrices \begin{equation}
\label{eq:B-modified}
\mathbf{\tilde{B}}_i = \mathbf{D}_i \mathbf{C}_i \left(\mathbf{I} - \mathbf{H}_{\mathbf{\ddot{U}}}\right) \boldsymbol\Phi \left(\mathbf{I} - \mathbf{H}_{\mathbf{\ddot{U}}}\right)'\mathbf{C}_i' \mathbf{D}_i'
\end{equation} and \begin{equation}
\label{eq:A-modified}
\mathbf{\tilde{A}}_i = \mathbf{D}_i' \mathbf{\tilde{B}}_i^{+1/2} \mathbf{D}_i.
\end{equation} Theorem 2 claimed that if the weight matrices are inverse
of the working model, such that
\(\mathbf{W}_i = \boldsymbol\Phi_i^{-1}\) for \(i = 1,...,m\), then
\(\mathbf{\tilde{B}}_i^{+1/2} = \mathbf{B}_i^{+1/2}\) and hence
\(\mathbf{\tilde{A}}_i = \mathbf{A}_i\). The implication is that the
cluster-specific fixed effects can be ignored when calculating the
adjustment matrices. However, the claimed equivalence does not hold in
general. The proof of Theorem 2 as given in the supplementary materials
of Pustejovsky and Tipton (2018) relied on a Woodbury identity for
generalized inverses that does not hold for \(\mathbf{B}_i\) because
necessary rank conditions are not satisfied.

We describe a simple numerical example that contradicts the original
statement of Theorem 2. Consider a design with \(m = 3\) clusters, of
sizes \(n_1 = 2\), \(n_2 = 3\), and \(n_3 = 5\) for which we have the
model \[
y_{it} = \beta_0 \times t + \mu_i + \epsilon_{it},
\] where the errors are heteroskedastic with
\(\text{Var}(\epsilon_{it}) = \alpha \times t\). We then have
\(\mathbf{y}_i = \left[ y_{i1} \cdots y_{in_i}\right]'\),
\(\mathbf{R}_i = \left[1 \ 2 \ \cdots \ n_i\right]'\),
\(\mathbf{T} = \bigoplus_{i=1}^3 \mathbf{1}_i\), and
\(\boldsymbol\Phi_i = \text{diag}(1,2,...,n_i)\).

If the model is estimated using inverse variance weights, so that
\(\mathbf{W}_i = \text{diag}\left(1, \frac{1}{2}, \cdots, \frac{1}{n_i}\right)\),
then the CR2 adjustment matrices differ depending on whether they are
calculated from the full model or from the model after absorbing the
fixed effects. Table \ref{tab:example} reports the product of the
adjustment matrices and the absorbed design matrix for the weighted
least squares estimator. The column labelled Full uses the adjustment
matrices based on the full design (i.e.,
\(\mathbf{A}_i \mathbf{W}_i \mathbf{\ddot{R}}_i\), with \(\mathbf{A}_i\)
calculated from Equation \ref{eq:A-matrix}) or based on the absorbed
design (i.e., \(\mathbf{\tilde{A}}_i \mathbf{W}_i \mathbf{\ddot{R}}_i\),
with \(\mathbf{\tilde{A}}_i\) calculated from Equation
\ref{eq:A-modified}). The values differ, contradicting the original
statement of Theorem 2. The final row of the table reports the value of
\(V^{CR2}\) based on fitting the model using the outcomes reported in
the second column of the table. The difference in adjustment matrices
leads to differences in the value of the variance estimator.

\begin{table}

\caption{\label{tab:example}Adjustment matrices based on weighted or unweighted least squares, calculated with or without absorbing fixed effects}
\centering
\begin{tabular}[t]{lrrrrrr}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{2}{c}{Weighted} & \multicolumn{1}{c}{Unweighted (Hom.)} & \multicolumn{2}{c}{Unweighted (Het.)} \\
\cmidrule(l{3pt}r{3pt}){3-4} \cmidrule(l{3pt}r{3pt}){5-5} \cmidrule(l{3pt}r{3pt}){6-7}
Cluster & Y & Full & Absorbed & Full/Absorbed & Full & Absorbed\\
\midrule
 & 1.6 & -0.110 & -0.342 & -0.510 & -0.334 & -0.497\\

\multirow[t]{-2}{*}{\raggedright\arraybackslash A} & 4.1 & 0.441 & 0.345 & 0.510 & 0.669 & 0.504\\

 & 2.6 & -0.174 & -0.689 & -1.091 & -0.658 & -0.997\\

 & 1.0 & 0.409 & 0.203 & 0.000 & 0.358 & 0.000\\

\multirow[t]{-3}{*}{\raggedright\arraybackslash B} & 7.6 & 0.647 & 0.518 & 1.091 & 1.437 & 1.072\\

 & 6.7 & -0.353 & -1.954 & -4.472 & -2.786 & -4.089\\

 & 5.0 & 0.483 & -0.176 & -2.236 & -0.770 & -2.342\\

 & 3.1 & 0.926 & 0.532 & 0.000 & 1.757 & 0.000\\

 & 3.7 & 1.193 & 0.925 & 2.236 & 4.487 & 2.564\\

\multirow[t]{-5}{*}{\raggedright\arraybackslash C} & 5.8 & 1.372 & 1.178 & 4.472 & 7.318 & 5.236\\
\midrule

$V^{CR2}$ &  & 0.828 & 1.019 & 1.173 & 1.248 & 1.050\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{a-revised-theorem-2}{%
\section{A revised Theorem 2}\label{a-revised-theorem-2}}

The implication of the original Theorem 2 was that using the modified
adjustment matrices \(\tilde{\mathbf{A}}_i\) to calculate the CR2
estimator yields the same result as using the full adjustment matrices
\(\mathbf{A}_i\). Although this does not hold under the general
conditions given above, a modified version of the theorem does hold for
the more limited case of ordinary (unweighted) least squares regression
with a homoskedastic working model. The precise conditions are given in
the following theorem, with proof given in Section \ref{proof}.

\begin{thm}
\label{thm:absorb}
Let $\mathbf{L}_i = \left(\mathbf{\ddot{U}}'\mathbf{\ddot{U}} - \mathbf{\ddot{U}}_i'\mathbf{\ddot{U}}_i\right)$ and assume that $\mathbf{L}_1,...,\mathbf{L}_m$ have full rank $r + s$. If $\mathbf{W}_i = \mathbf{I}_i$ and $\boldsymbol\Phi_i = \mathbf{I}_i$ for $i = 1,...,m$ and $\mathbf{T}_i \mathbf{T}_k' = \mathbf{0}$ for $i \neq k$, then $\mathbf{A}_i \mathbf{\ddot{R}}_i = \mathbf{\tilde{A}}_i \mathbf{\ddot{R}}_i$, where $\mathbf{A}_i$ and $\tilde{\mathbf{A}}_i$ are as defined in (\ref{eq:A-matrix}) and (\ref{eq:A-modified}), respectively.
\end{thm}

The implication of the revised theorem is that, for ordinary least
squares regression with a homoskedastic working model, calculating the
CR2 with the modified adjustment matrices \(\tilde{\mathbf{A}}_i\) leads
to the same result as using the full adjustment matrices
\(\mathbf{A}_i\). The equality between the full and absorbed adjustment
matrices does not hold for weighted or generalized least squares, nor
for ordinary least squares with working models other than
\(\boldsymbol\Phi_i = \mathbf{I}_i\).

Continuing the example described in the previous section, Table
\ref{tab:example} reports the product of the adjustment matrices and the
absorbed design matrix for the ordinary least squares estimator with a
homoskedastic working model in the column labelled Unweighted (Hom.).
The values based on the full design matrix
\(\left(\mathbf{A}_i \mathbf{\ddot{R}}_i\right)\) are numerically
identical to the values based on the absorbed design matrix
\(\left(\mathbf{\tilde{A}}_i \mathbf{\ddot{R}}_i\right)\). In the
columns labeled Unweighted (Het.), the same quantities are computed for
the ordinary least squares estimator, but with the heteroskedastic
working model described in the previous section. The values based on the
full design matrix differ from the values based on the absorbed design
matrix, leading to differences in the cluster-robust variance estimator
reported in the final row of the table.

\hypertarget{remarks}{%
\subsection{Remarks}\label{remarks}}

The revised version of Theorem 2 holds for the class of linear models
estimated using ordinary least squares, with the cluster-robust variance
estimator constructed based on a homoskedastic working model.
Considering the ubiquity of ordinary least squares in applied data
analysis, this is clearly an important class of estimators---perhaps
even the most important for application. Indeed, recent methodological
work on small-sample adjustments to cluster-robust variance estimators
has focused almost exclusively on ordinary least squares (e.g.,
Ibragimov and Müller 2016; Imbens and Kolesár 2016; MacKinnon and Webb
2016).

Other important classes of estimators fall outside the scope of Theorem
2. For instance, an analyst might prefer to estimate a linear model
using weighted least squares based on a posited heteroskedastic variance
structure, paired with heteroskedasticity- or cluster-robust variance
estimators to buttress against misspecification of that variance
structure (Romano and Wolf 2017). In other applications, an analyst
might use generalized estimating equation with a compound symmetric or
auto-regressive working model for the errors (Liang and Zeger 1986; Wang
and Carey 2003). In still other applications, analysts might need to
estimate a linear model using survey weights or inverse propensity
weights, while maintaining a homoskedastic working model. For such
applications, efficient computation of CR2 or other small-sample
adjusted cluster-robust variance estimators remains a topic for further
research.

\hypertarget{proof}{%
\subsection{Proof}\label{proof}}

Setting \(\boldsymbol\Phi_i = \mathbf{I}_i\) and observing that
\(\mathbf{\ddot{U}}_i'\mathbf{T}_i = \mathbf{0}\) for \(i = 1,...,m\),
it follows that \begin{align}
\mathbf{B}_i &= \mathbf{D}_i \mathbf{C}_i \left(\mathbf{I} - \mathbf{H_{\ddot{U}}}\right) \left(\mathbf{I} - \mathbf{H_T}\right) \boldsymbol\Phi \left(\mathbf{I} - \mathbf{H_T}\right)' \left(\mathbf{I} - \mathbf{H_{\ddot{U}}}\right)' \mathbf{C}_i' \mathbf{D}_i' \nonumber \\ 
&= \mathbf{C}_i \left(\mathbf{I} - \mathbf{H_{\ddot{U}}} - \mathbf{H_T}\right) \left(\mathbf{I} - \mathbf{H_{\ddot{U}}} - \mathbf{H_T}\right) \mathbf{C}_i' \nonumber\\ 
\label{eq:B_i}
&= \left(\mathbf{I}_i - \mathbf{\ddot{U}}_i \mathbf{M_{\ddot{U}}}\mathbf{\ddot{U}}_i' - \mathbf{T}_i \mathbf{M_T}\mathbf{T}_i'\right)
\end{align} and similarly, \begin{equation}
\label{eq:Btilde_i}
\tilde{\mathbf{B}}_i = \left(\mathbf{I}_i - \mathbf{\ddot{U}}_i \mathbf{M_{\ddot{U}}}\mathbf{\ddot{U}}_i'\right).
\end{equation}

It is apparent that
\(\tilde{\mathbf{B}}_i \mathbf{T}_i = \mathbf{T}_i\). We now show that
\(\tilde{\mathbf{A}}_i \mathbf{T}_i = \mathbf{T}_i\) as well. Denote the
rank of \(\mathbf{\ddot{U}}_i\) as
\(u_i \leq \min \left\{n_i, r + s \right\}\) and take the thin QR
decomposition of \(\mathbf{\ddot{U}}_i\) as
\(\mathbf{\ddot{U}}_i = \mathbf{Q}_i \mathbf{R}_i\), where
\(\mathbf{Q}_i\) is an \(n_i \times u_i\) semi-orthonormal matrix and
\(\mathbf{R}_i\) is a \(u_i \times r + s\) matrix of rank \(u_i\), with
\(\mathbf{Q}_i'\mathbf{Q}_i = \mathbf{I}\). Note that
\(\mathbf{Q}_i'\mathbf{T}_i = \mathbf{0}\). From the observation that
\(\tilde{\mathbf{B}}_i\) can be written as \[
\tilde{\mathbf{B}}_i = \mathbf{I}_i - \mathbf{Q}_i \mathbf{Q}_i' + \mathbf{Q}_i \left(\mathbf{I} - \mathbf{R}_i \mathbf{M}_{\mathbf{\ddot{U}}} \mathbf{R}_i'\right)\mathbf{Q}_i',
\] it can be seen that \begin{equation}
\tilde{\mathbf{A}}_i = \tilde{\mathbf{B}}_i^{+1/2} = \mathbf{I}_i - \mathbf{Q}_i \mathbf{Q}_i' + \mathbf{Q}_i \left(\mathbf{I} - \mathbf{R}_i \mathbf{M}_{\mathbf{\ddot{U}}} \mathbf{R}_i'\right)^{+1/2} \mathbf{Q}_i'.
\end{equation} It follows that
\(\tilde{\mathbf{A}}_i \mathbf{T}_i = \mathbf{T}_i\).

Define the matrices \begin{equation}
\mathbf{A}_i = \tilde{\mathbf{A}}_i - \mathbf{T}_i \mathbf{M_T}\mathbf{T}_i',
\end{equation} for \(i = 1,...,m\). We claim that
\(\mathbf{A}_i = \mathbf{B}_i^{+1/2}\). This can be seen by observing
that \(\mathbf{A}_i \mathbf{A}_i\) is equal to \(\mathbf{B}_i^+\), the
Moore-Penrose inverse of \(\mathbf{B}_i\). Note that \begin{equation}
\mathbf{A}_i \mathbf{A}_i = \tilde{\mathbf{A}}_i \tilde{\mathbf{A}}_i - \mathbf{T}_i \mathbf{M_T}\mathbf{T}_i' =  = \tilde{\mathbf{B}}_i^+ - \mathbf{T}_i \mathbf{M_T}\mathbf{T}_i'.
\end{equation} It can then readily be verified that i)
\(\mathbf{B}_i \mathbf{A}_i \mathbf{A}_i \mathbf{B}_i = \mathbf{B}_i\),
ii)
\(\mathbf{A}_i \mathbf{A}_i \mathbf{B}_i \mathbf{A}_i \mathbf{A}_i = \mathbf{A}_i \mathbf{A}_i\),
and iii)
\(\mathbf{A}_i \mathbf{A}_i \mathbf{B}_i = \mathbf{B}_i \mathbf{A}_i \mathbf{A}_i\).
Thus, \(\mathbf{A}_i\mathbf{A}_i\) satisfies the definition of the
Moore-Penrose inverse of \(\mathbf{B}_i\) (Rao and Mitra 1971).

Finally, because \(\mathbf{T}_i ' \mathbf{\ddot{R}}_i= \mathbf{0}\), it
can be seen that
\(\mathbf{A}_i \mathbf{\ddot{R}}_i = \left(\tilde{\mathbf{A}}_i - \mathbf{T}_i \mathbf{M_T}\mathbf{T}_i'\right)\mathbf{\ddot{R}}_i = \tilde{\mathbf{A}}_i \mathbf{\ddot{R}}_i\).

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-Bell2002bias}{}}%
Bell, Robert M, and Daniel F McCaffrey. 2002. {``{Bias reduction in
standard errors for linear regression with multi-stage samples}.''}
\emph{Survey Methodology} 28 (2): 169--81.

\leavevmode\vadjust pre{\hypertarget{ref-Ibragimov2016inference}{}}%
Ibragimov, Rustam, and Ulrich K Müller. 2016. {``{Inference with few
heterogeneous clusters}.''} \emph{Review of Economics and Statistics} 98
(1): 83--96. \url{https://doi.org/10.1162/REST_a_00545}.

\leavevmode\vadjust pre{\hypertarget{ref-Imbens2015robust}{}}%
Imbens, Guido W, and Michal Kolesár. 2016. {``Robust Standard Errors in
Small Samples: Some Practical Advice.''} \emph{Review of Economics and
Statistics} 98 (4): 701--12. \url{https://doi.org/10.1162/REST_a_00552}.

\leavevmode\vadjust pre{\hypertarget{ref-Liang1986longitudinal}{}}%
Liang, Kung-Yee, and Scott L Zeger. 1986. {``{Longitudinal data analysis
using generalized linear models}.''} \emph{Biometrika} 73 (1): 13--22.

\leavevmode\vadjust pre{\hypertarget{ref-MacKinnon2016wild}{}}%
MacKinnon, James G, and Matthew D Webb. 2016. {``{Wild bootstrap
inference for wildly different cluster sizes}.''} \emph{Journal of
Applied Econometrics} forthcoming.
\url{https://doi.org/10.1002/jae.2508}.

\leavevmode\vadjust pre{\hypertarget{ref-McCaffrey2001generalizations}{}}%
McCaffrey, Daniel F, Robert M Bell, and Carsten H Botts. 2001.
{``{Generalizations of biased reduced linearization}.''} In
\emph{Proceedings of the Annual Meeting of the American Statistical
Association}. 1994.

\leavevmode\vadjust pre{\hypertarget{ref-pustejovsky2018small}{}}%
Pustejovsky, James E, and Elizabeth Tipton. 2018. {``Small-Sample
Methods for Cluster-Robust Variance Estimation and Hypothesis Testing in
Fixed Effects Models.''} \emph{Journal of Business \& Economic
Statistics} 36 (4): 672--83.
\url{https://doi.org/10.1080/07350015.2016.1247004}.

\leavevmode\vadjust pre{\hypertarget{ref-rao1971generalized}{}}%
Rao, C. Radhakrishna, and Sujit Kumar Mitra. 1971. \emph{{Generalized
Inverse of Matrices and its Applications}}. New York, NY: John Wiley \&
Sons.

\leavevmode\vadjust pre{\hypertarget{ref-romano2017resurrecting}{}}%
Romano, Joseph P., and Michael Wolf. 2017. {``Resurrecting Weighted
Least Squares.''} \emph{Journal of Econometrics} 197 (1): 1--19.
\url{https://doi.org/10.1016/j.jeconom.2016.10.003}.

\leavevmode\vadjust pre{\hypertarget{ref-wang2003working}{}}%
Wang, Y.-G., and Vincent J Carey. 2003. {``Working Correlation Structure
Misspecification, Estimation and Covariate Design: Implications for
Generalised Estimating Equations Performance.''} \emph{Biometrika} 90
(1): 29--41. \url{https://doi.org/10.1093/biomet/90.1.29}.

\end{CSLReferences}

\bibliographystyle{agsm}
\bibliography{bibliography.bib}


\end{document}
